{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"fff9c3f75f2b4b87925860f40fafe163","deepnote_cell_type":"text-cell-p"},"source":"### 2a\r\n* 3 lagen 1 input 1 hidden 1 output.\r\n    * 3 input neurons vanwegen 3 features\r\n    * vanwegen de duimregel mean van input en output. en de duimregel even getallen. pakken we 4 neurons in hidden layer.\r\n    * 6 output vanwege 6 verschillende klasse\r\n    * vanwege simpele data set zijn meer hidden layers niet nodig.","block_group":"30ea3de4f488456687ad0ed97f914d29"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3752b5a2666c465a953c45f2a6facc48","deepnote_cell_type":"text-cell-p"},"source":"* activation\r\n    * eindigen op softmax\r\n    * relu voor andere lagen.\r\n* cost function\r\n    * categorial cross entropy\r\n* optimizer\r\n    * Adam\r\n* numbers of epoch.\r\n    * epochs zijn in het begin ingesteld op 15.\r\n* Batch size\r\n    * 20/80 split voor test en train\r\n    * train\r\n        * delen in 4. batches van 100\r\n* Learning rate\r\n    * 0.05\r\n","block_group":"d04f989c55ac490ca5904cc950e4bd25"},{"cell_type":"code","metadata":{"source_hash":"a82c062","execution_start":1727701307920,"execution_millis":2920,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"585e1512e5ea4ec0b690d829f7dd8e13","deepnote_cell_type":"code"},"source":"!pip install optuna","block_group":"b0003f1ba1354567a53d6041981be961","execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /root/venv/lib/python3.9/site-packages (4.0.0)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.9/site-packages (from optuna) (6.0.2)\nRequirement already satisfied: colorlog in /root/venv/lib/python3.9/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: alembic>=1.5.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from optuna) (1.11.1)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from optuna) (21.3)\nRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from optuna) (4.64.1)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from optuna) (1.23.4)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from optuna) (1.4.42)\nRequirement already satisfied: typing-extensions>=4 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.4.0)\nRequirement already satisfied: Mako in /shared-libs/python3.9/py/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.9)\nRequirement already satisfied: greenlet!=0.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3.post0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/d909f718-f82a-494c-8f25-6c7cf4132303","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ba2b5b20","execution_start":1727701310888,"execution_millis":2552,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"5365ba9d3b124e65846ec6ab58e5b845","deepnote_cell_type":"code"},"source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\ndf = pd.read_csv('bmi.csv', sep=',')\ndf = df.sample(frac=1).reset_index(drop=True)\nfrom sklearn.model_selection import train_test_split\ndef convert_sex(sex):\n    if(sex == 'Male'):\n        return 1\n    else:\n        return 0\ndf['Gender'] = df['Gender'].apply(convert_sex)\n\n\nX = df.drop(columns=['Index'])\ny = df['Index']  # Target column\n\ndf.head()","block_group":"5365ba9d3b124e65846ec6ab58e5b845","execution_count":2,"outputs":[{"name":"stderr","text":"2024-09-30 13:01:51.263241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-09-30 13:01:51.404714: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-09-30 13:01:51.404762: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-09-30 13:01:51.435689: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-09-30 13:01:52.134182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-09-30 13:01:52.134276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-09-30 13:01:52.134287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":5,"columns":[{"name":"Gender","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Height","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":"164","max":"198","histogram":[{"bin_start":164,"bin_end":167.4,"count":1},{"bin_start":167.4,"bin_end":170.8,"count":0},{"bin_start":170.8,"bin_end":174.2,"count":1},{"bin_start":174.2,"bin_end":177.6,"count":0},{"bin_start":177.6,"bin_end":181,"count":0},{"bin_start":181,"bin_end":184.4,"count":1},{"bin_start":184.4,"bin_end":187.8,"count":1},{"bin_start":187.8,"bin_end":191.2,"count":0},{"bin_start":191.2,"bin_end":194.6,"count":0},{"bin_start":194.6,"bin_end":198,"count":1}]}},{"name":"Weight","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":"50","max":"160","histogram":[{"bin_start":50,"bin_end":61,"count":1},{"bin_start":61,"bin_end":72,"count":0},{"bin_start":72,"bin_end":83,"count":0},{"bin_start":83,"bin_end":94,"count":0},{"bin_start":94,"bin_end":105,"count":0},{"bin_start":105,"bin_end":116,"count":1},{"bin_start":116,"bin_end":127,"count":1},{"bin_start":127,"bin_end":138,"count":0},{"bin_start":138,"bin_end":149,"count":1},{"bin_start":149,"bin_end":160,"count":1}]}},{"name":"Index","dtype":"int64","stats":{"unique_count":3,"nan_count":0,"min":"0","max":"5","histogram":[{"bin_start":0,"bin_end":0.5,"count":1},{"bin_start":0.5,"bin_end":1,"count":0},{"bin_start":1,"bin_end":1.5,"count":0},{"bin_start":1.5,"bin_end":2,"count":0},{"bin_start":2,"bin_end":2.5,"count":0},{"bin_start":2.5,"bin_end":3,"count":0},{"bin_start":3,"bin_end":3.5,"count":0},{"bin_start":3.5,"bin_end":4,"count":0},{"bin_start":4,"bin_end":4.5,"count":1},{"bin_start":4.5,"bin_end":5,"count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Gender":0,"Height":171,"Weight":120,"Index":5,"_deepnote_index_column":0},{"Gender":0,"Height":164,"Weight":160,"Index":5,"_deepnote_index_column":1},{"Gender":0,"Height":186,"Weight":146,"Index":5,"_deepnote_index_column":2},{"Gender":0,"Height":181,"Weight":106,"Index":4,"_deepnote_index_column":3},{"Gender":0,"Height":198,"Weight":50,"Index":0,"_deepnote_index_column":4}]},"text/plain":"   Gender  Height  Weight  Index\n0       0     171     120      5\n1       0     164     160      5\n2       0     186     146      5\n3       0     181     106      4\n4       0     198      50      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>Index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>171</td>\n      <td>120</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>164</td>\n      <td>160</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>186</td>\n      <td>146</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>181</td>\n      <td>106</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>198</td>\n      <td>50</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/776ed33e-1390-46a2-9b92-630c5de206b0","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"3ea93a31","execution_start":1727701313489,"execution_millis":0,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"cc736f1285bb43a1995f607f4dd216eb","deepnote_cell_type":"code"},"source":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=df['Index'], random_state=42)\n\nprint(df.dtypes)  # Controleer de types van je kolommen\n\ny_train = to_categorical(y_train, num_classes=6)  # Assuming 6 classes\ny_test = to_categorical(y_test, num_classes=6)\n\nprint(X_train.shape)\nprint(y_train.shape)\n","block_group":"18d8cc12964240f4956c68c88cd01e68","execution_count":3,"outputs":[{"name":"stdout","text":"Gender    int64\nHeight    int64\nWeight    int64\nIndex     int64\ndtype: object\n(400, 3)\n(400, 6)\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ab1bcda3-5a4b-4319-8092-9c29ff3324b6","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"94ecda56","execution_start":1727701313577,"execution_millis":5957,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"d7374664aa164ddd819755a50f370661","deepnote_cell_type":"code"},"source":"model = Sequential()\nmodel.add(Dense(4, input_shape=(X_train.shape[1],), activation='relu'))\nmodel.add(Dense(6, activation='softmax'))\n\nfrom keras.metrics import Recall\nfrom tensorflow.keras.metrics import AUC\n\nmodel.compile(Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=[AUC()])\nmodel.fit(X_train, y_train, batch_size=100, epochs=150, validation_split=0.1)\n\n","block_group":"f34cdd99339a48b1b05c54a608a7e084","execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/150\n2024-09-30 13:01:53.503956: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2024-09-30 13:01:53.503993: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2024-09-30 13:01:53.504013: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-70241bed-abd0-479d-8376-545af7db2871): /proc/driver/nvidia/version does not exist\n2024-09-30 13:01:53.504253: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n4/4 [==============================] - 1s 79ms/step - loss: 57.9031 - auc: 0.4833 - val_loss: 50.1395 - val_auc: 0.4750\nEpoch 2/150\n4/4 [==============================] - 0s 6ms/step - loss: 44.5812 - auc: 0.4833 - val_loss: 37.7709 - val_auc: 0.4750\nEpoch 3/150\n4/4 [==============================] - 0s 11ms/step - loss: 32.8036 - auc: 0.4833 - val_loss: 27.1089 - val_auc: 0.4750\nEpoch 4/150\n4/4 [==============================] - 0s 6ms/step - loss: 22.9081 - auc: 0.4439 - val_loss: 18.5881 - val_auc: 0.3973\nEpoch 5/150\n4/4 [==============================] - 0s 6ms/step - loss: 15.3528 - auc: 0.3852 - val_loss: 12.5329 - val_auc: 0.3703\nEpoch 6/150\n4/4 [==============================] - 0s 18ms/step - loss: 10.5364 - auc: 0.3927 - val_loss: 8.9794 - val_auc: 0.3773\nEpoch 7/150\n4/4 [==============================] - 0s 14ms/step - loss: 7.0052 - auc: 0.4581 - val_loss: 5.9979 - val_auc: 0.4384\nEpoch 8/150\n4/4 [==============================] - 0s 9ms/step - loss: 5.0023 - auc: 0.5076 - val_loss: 4.9306 - val_auc: 0.4021\nEpoch 9/150\n4/4 [==============================] - 0s 8ms/step - loss: 3.9480 - auc: 0.4879 - val_loss: 3.2812 - val_auc: 0.4229\nEpoch 10/150\n4/4 [==============================] - 0s 8ms/step - loss: 2.6216 - auc: 0.5124 - val_loss: 1.9387 - val_auc: 0.6076\nEpoch 11/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.7083 - auc: 0.6637 - val_loss: 1.5168 - val_auc: 0.7300\nEpoch 12/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.7243 - auc: 0.7321 - val_loss: 1.7362 - val_auc: 0.7220\nEpoch 13/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.9429 - auc: 0.7132 - val_loss: 1.7313 - val_auc: 0.7066\nEpoch 14/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.8428 - auc: 0.7085 - val_loss: 1.5742 - val_auc: 0.7078\nEpoch 15/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.6308 - auc: 0.7265 - val_loss: 1.4883 - val_auc: 0.7421\nEpoch 16/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.5260 - auc: 0.7421 - val_loss: 1.5261 - val_auc: 0.7471\nEpoch 17/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.5346 - auc: 0.7269 - val_loss: 1.5825 - val_auc: 0.7185\nEpoch 18/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.5562 - auc: 0.7258 - val_loss: 1.5853 - val_auc: 0.7088\nEpoch 19/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.5444 - auc: 0.7279 - val_loss: 1.5413 - val_auc: 0.6931\nEpoch 20/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.5172 - auc: 0.7311 - val_loss: 1.4915 - val_auc: 0.7529\nEpoch 21/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.5073 - auc: 0.7431 - val_loss: 1.4686 - val_auc: 0.7504\nEpoch 22/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.5090 - auc: 0.7468 - val_loss: 1.4646 - val_auc: 0.7504\nEpoch 23/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.5116 - auc: 0.7418 - val_loss: 1.4646 - val_auc: 0.7506\nEpoch 24/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.5081 - auc: 0.7458 - val_loss: 1.4676 - val_auc: 0.7422\nEpoch 25/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.5040 - auc: 0.7412 - val_loss: 1.4756 - val_auc: 0.7479\nEpoch 26/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.5009 - auc: 0.7459 - val_loss: 1.4823 - val_auc: 0.7502\nEpoch 27/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.5019 - auc: 0.7471 - val_loss: 1.4869 - val_auc: 0.7544\nEpoch 28/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.5027 - auc: 0.7475 - val_loss: 1.4844 - val_auc: 0.7493\nEpoch 29/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.5016 - auc: 0.7469 - val_loss: 1.4777 - val_auc: 0.7548\nEpoch 30/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.5006 - auc: 0.7475 - val_loss: 1.4742 - val_auc: 0.7521\nEpoch 31/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.5002 - auc: 0.7471 - val_loss: 1.4732 - val_auc: 0.7485\nEpoch 32/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.5009 - auc: 0.7497 - val_loss: 1.4738 - val_auc: 0.7423\nEpoch 33/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4999 - auc: 0.7474 - val_loss: 1.4730 - val_auc: 0.7446\nEpoch 34/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.5002 - auc: 0.7458 - val_loss: 1.4740 - val_auc: 0.7457\nEpoch 35/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.5009 - auc: 0.7444 - val_loss: 1.4724 - val_auc: 0.7434\nEpoch 36/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.5005 - auc: 0.7461 - val_loss: 1.4746 - val_auc: 0.7437\nEpoch 37/150\n4/4 [==============================] - 0s 10ms/step - loss: 1.4998 - auc: 0.7458 - val_loss: 1.4754 - val_auc: 0.7441\nEpoch 38/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.4996 - auc: 0.7464 - val_loss: 1.4758 - val_auc: 0.7469\nEpoch 39/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.5002 - auc: 0.7433 - val_loss: 1.4711 - val_auc: 0.7439\nEpoch 40/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4992 - auc: 0.7463 - val_loss: 1.4757 - val_auc: 0.7514\nEpoch 41/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4992 - auc: 0.7486 - val_loss: 1.4751 - val_auc: 0.7448\nEpoch 42/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4985 - auc: 0.7504 - val_loss: 1.4735 - val_auc: 0.7527\nEpoch 43/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.4985 - auc: 0.7518 - val_loss: 1.4710 - val_auc: 0.7454\nEpoch 44/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4987 - auc: 0.7492 - val_loss: 1.4690 - val_auc: 0.7559\nEpoch 45/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4981 - auc: 0.7498 - val_loss: 1.4725 - val_auc: 0.7494\nEpoch 46/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.4980 - auc: 0.7489 - val_loss: 1.4757 - val_auc: 0.7457\nEpoch 47/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4994 - auc: 0.7440 - val_loss: 1.4724 - val_auc: 0.7419\nEpoch 48/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4969 - auc: 0.7502 - val_loss: 1.4736 - val_auc: 0.7508\nEpoch 49/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.4971 - auc: 0.7504 - val_loss: 1.4715 - val_auc: 0.7503\nEpoch 50/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4974 - auc: 0.7483 - val_loss: 1.4735 - val_auc: 0.7492\nEpoch 51/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4969 - auc: 0.7528 - val_loss: 1.4702 - val_auc: 0.7442\nEpoch 52/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.4965 - auc: 0.7529 - val_loss: 1.4709 - val_auc: 0.7486\nEpoch 53/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4968 - auc: 0.7488 - val_loss: 1.4696 - val_auc: 0.7464\nEpoch 54/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4963 - auc: 0.7469 - val_loss: 1.4705 - val_auc: 0.7414\nEpoch 55/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.4967 - auc: 0.7480 - val_loss: 1.4666 - val_auc: 0.7424\nEpoch 56/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4958 - auc: 0.7495 - val_loss: 1.4662 - val_auc: 0.7523\nEpoch 57/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4949 - auc: 0.7533 - val_loss: 1.4684 - val_auc: 0.7462\nEpoch 58/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.4962 - auc: 0.7482 - val_loss: 1.4716 - val_auc: 0.7523\nEpoch 59/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4956 - auc: 0.7540 - val_loss: 1.4711 - val_auc: 0.7498\nEpoch 60/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4944 - auc: 0.7538 - val_loss: 1.4709 - val_auc: 0.7463\nEpoch 61/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4953 - auc: 0.7519 - val_loss: 1.4694 - val_auc: 0.7560\nEpoch 62/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4954 - auc: 0.7508 - val_loss: 1.4681 - val_auc: 0.7568\nEpoch 63/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4948 - auc: 0.7536 - val_loss: 1.4649 - val_auc: 0.7533\nEpoch 64/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4938 - auc: 0.7560 - val_loss: 1.4659 - val_auc: 0.7536\nEpoch 65/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4935 - auc: 0.7574 - val_loss: 1.4684 - val_auc: 0.7530\nEpoch 66/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4942 - auc: 0.7545 - val_loss: 1.4702 - val_auc: 0.7503\nEpoch 67/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4943 - auc: 0.7533 - val_loss: 1.4665 - val_auc: 0.7593\nEpoch 68/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4931 - auc: 0.7541 - val_loss: 1.4672 - val_auc: 0.7575\nEpoch 69/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4927 - auc: 0.7536 - val_loss: 1.4656 - val_auc: 0.7572\nEpoch 70/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4925 - auc: 0.7551 - val_loss: 1.4668 - val_auc: 0.7573\nEpoch 71/150\n4/4 [==============================] - 0s 12ms/step - loss: 1.4924 - auc: 0.7555 - val_loss: 1.4647 - val_auc: 0.7566\nEpoch 72/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4925 - auc: 0.7545 - val_loss: 1.4668 - val_auc: 0.7543\nEpoch 73/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4922 - auc: 0.7560 - val_loss: 1.4673 - val_auc: 0.7546\nEpoch 74/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4912 - auc: 0.7580 - val_loss: 1.4630 - val_auc: 0.7581\nEpoch 75/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4916 - auc: 0.7573 - val_loss: 1.4597 - val_auc: 0.7578\nEpoch 76/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4912 - auc: 0.7572 - val_loss: 1.4632 - val_auc: 0.7543\nEpoch 77/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4908 - auc: 0.7568 - val_loss: 1.4676 - val_auc: 0.7530\nEpoch 78/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4908 - auc: 0.7577 - val_loss: 1.4676 - val_auc: 0.7576\nEpoch 79/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.4898 - auc: 0.7607 - val_loss: 1.4626 - val_auc: 0.7588\nEpoch 80/150\n4/4 [==============================] - 0s 10ms/step - loss: 1.4911 - auc: 0.7570 - val_loss: 1.4556 - val_auc: 0.7605\nEpoch 81/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4909 - auc: 0.7603 - val_loss: 1.4552 - val_auc: 0.7671\nEpoch 82/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.4910 - auc: 0.7587 - val_loss: 1.4620 - val_auc: 0.7644\nEpoch 83/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4887 - auc: 0.7590 - val_loss: 1.4677 - val_auc: 0.7531\nEpoch 84/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4905 - auc: 0.7579 - val_loss: 1.4679 - val_auc: 0.7616\nEpoch 85/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.4884 - auc: 0.7606 - val_loss: 1.4577 - val_auc: 0.7598\nEpoch 86/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4890 - auc: 0.7584 - val_loss: 1.4548 - val_auc: 0.7631\nEpoch 87/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4895 - auc: 0.7580 - val_loss: 1.4565 - val_auc: 0.7590\nEpoch 88/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4871 - auc: 0.7632 - val_loss: 1.4621 - val_auc: 0.7575\nEpoch 89/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4878 - auc: 0.7626 - val_loss: 1.4641 - val_auc: 0.7544\nEpoch 90/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4885 - auc: 0.7626 - val_loss: 1.4690 - val_auc: 0.7556\nEpoch 91/150\n4/4 [==============================] - 0s 12ms/step - loss: 1.4895 - auc: 0.7599 - val_loss: 1.4657 - val_auc: 0.7633\nEpoch 92/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4867 - auc: 0.7635 - val_loss: 1.4602 - val_auc: 0.7595\nEpoch 93/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4869 - auc: 0.7615 - val_loss: 1.4596 - val_auc: 0.7590\nEpoch 94/150\n4/4 [==============================] - 0s 13ms/step - loss: 1.4852 - auc: 0.7648 - val_loss: 1.4598 - val_auc: 0.7529\nEpoch 95/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4872 - auc: 0.7617 - val_loss: 1.4581 - val_auc: 0.7694\nEpoch 96/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4855 - auc: 0.7648 - val_loss: 1.4577 - val_auc: 0.7717\nEpoch 97/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4857 - auc: 0.7643 - val_loss: 1.4570 - val_auc: 0.7616\nEpoch 98/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4835 - auc: 0.7669 - val_loss: 1.4596 - val_auc: 0.7656\nEpoch 99/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4836 - auc: 0.7650 - val_loss: 1.4598 - val_auc: 0.7701\nEpoch 100/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.4844 - auc: 0.7643 - val_loss: 1.4579 - val_auc: 0.7689\nEpoch 101/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4834 - auc: 0.7663 - val_loss: 1.4527 - val_auc: 0.7673\nEpoch 102/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4831 - auc: 0.7676 - val_loss: 1.4494 - val_auc: 0.7633\nEpoch 103/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4840 - auc: 0.7676 - val_loss: 1.4509 - val_auc: 0.7661\nEpoch 104/150\n4/4 [==============================] - 0s 11ms/step - loss: 1.4833 - auc: 0.7664 - val_loss: 1.4664 - val_auc: 0.7729\nEpoch 105/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4847 - auc: 0.7648 - val_loss: 1.4679 - val_auc: 0.7713\nEpoch 106/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.4823 - auc: 0.7630 - val_loss: 1.4526 - val_auc: 0.7709\nEpoch 107/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4833 - auc: 0.7652 - val_loss: 1.4489 - val_auc: 0.7740\nEpoch 108/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4841 - auc: 0.7672 - val_loss: 1.4508 - val_auc: 0.7647\nEpoch 109/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4801 - auc: 0.7704 - val_loss: 1.4546 - val_auc: 0.7679\nEpoch 110/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4795 - auc: 0.7707 - val_loss: 1.4559 - val_auc: 0.7671\nEpoch 111/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4794 - auc: 0.7698 - val_loss: 1.4548 - val_auc: 0.7699\nEpoch 112/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4802 - auc: 0.7689 - val_loss: 1.4521 - val_auc: 0.7672\nEpoch 113/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4797 - auc: 0.7711 - val_loss: 1.4520 - val_auc: 0.7724\nEpoch 114/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4801 - auc: 0.7708 - val_loss: 1.4486 - val_auc: 0.7685\nEpoch 115/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4769 - auc: 0.7741 - val_loss: 1.4524 - val_auc: 0.7719\nEpoch 116/150\n4/4 [==============================] - 0s 10ms/step - loss: 1.4766 - auc: 0.7719 - val_loss: 1.4630 - val_auc: 0.7717\nEpoch 117/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.4793 - auc: 0.7704 - val_loss: 1.4594 - val_auc: 0.7756\nEpoch 118/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4777 - auc: 0.7713 - val_loss: 1.4461 - val_auc: 0.7736\nEpoch 119/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4760 - auc: 0.7739 - val_loss: 1.4474 - val_auc: 0.7754\nEpoch 120/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4744 - auc: 0.7743 - val_loss: 1.4455 - val_auc: 0.7782\nEpoch 121/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4739 - auc: 0.7739 - val_loss: 1.4492 - val_auc: 0.7751\nEpoch 122/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4758 - auc: 0.7680 - val_loss: 1.4495 - val_auc: 0.7780\nEpoch 123/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.4739 - auc: 0.7709 - val_loss: 1.4487 - val_auc: 0.7759\nEpoch 124/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4733 - auc: 0.7758 - val_loss: 1.4476 - val_auc: 0.7736\nEpoch 125/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4724 - auc: 0.7751 - val_loss: 1.4454 - val_auc: 0.7728\nEpoch 126/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4719 - auc: 0.7754 - val_loss: 1.4446 - val_auc: 0.7783\nEpoch 127/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4722 - auc: 0.7716 - val_loss: 1.4419 - val_auc: 0.7797\nEpoch 128/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4694 - auc: 0.7743 - val_loss: 1.4450 - val_auc: 0.7786\nEpoch 129/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4712 - auc: 0.7773 - val_loss: 1.4487 - val_auc: 0.7779\nEpoch 130/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4697 - auc: 0.7786 - val_loss: 1.4404 - val_auc: 0.7800\nEpoch 131/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4690 - auc: 0.7755 - val_loss: 1.4429 - val_auc: 0.7795\nEpoch 132/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4675 - auc: 0.7754 - val_loss: 1.4431 - val_auc: 0.7779\nEpoch 133/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4704 - auc: 0.7702 - val_loss: 1.4462 - val_auc: 0.7836\nEpoch 134/150\n4/4 [==============================] - 0s 7ms/step - loss: 1.4669 - auc: 0.7749 - val_loss: 1.4336 - val_auc: 0.7744\nEpoch 135/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4663 - auc: 0.7806 - val_loss: 1.4359 - val_auc: 0.7768\nEpoch 136/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4654 - auc: 0.7804 - val_loss: 1.4384 - val_auc: 0.7806\nEpoch 137/150\n4/4 [==============================] - 0s 15ms/step - loss: 1.4642 - auc: 0.7807 - val_loss: 1.4381 - val_auc: 0.7812\nEpoch 138/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4631 - auc: 0.7813 - val_loss: 1.4406 - val_auc: 0.7840\nEpoch 139/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4626 - auc: 0.7817 - val_loss: 1.4446 - val_auc: 0.7848\nEpoch 140/150\n4/4 [==============================] - 0s 11ms/step - loss: 1.4615 - auc: 0.7815 - val_loss: 1.4387 - val_auc: 0.7846\nEpoch 141/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4609 - auc: 0.7812 - val_loss: 1.4353 - val_auc: 0.7846\nEpoch 142/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4598 - auc: 0.7831 - val_loss: 1.4299 - val_auc: 0.7843\nEpoch 143/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.4604 - auc: 0.7825 - val_loss: 1.4276 - val_auc: 0.7831\nEpoch 144/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4582 - auc: 0.7823 - val_loss: 1.4331 - val_auc: 0.7851\nEpoch 145/150\n4/4 [==============================] - 0s 9ms/step - loss: 1.4590 - auc: 0.7793 - val_loss: 1.4405 - val_auc: 0.7890\nEpoch 146/150\n4/4 [==============================] - 0s 16ms/step - loss: 1.4575 - auc: 0.7819 - val_loss: 1.4356 - val_auc: 0.7867\nEpoch 147/150\n4/4 [==============================] - 0s 8ms/step - loss: 1.4574 - auc: 0.7795 - val_loss: 1.4309 - val_auc: 0.7897\nEpoch 148/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4548 - auc: 0.7826 - val_loss: 1.4265 - val_auc: 0.7893\nEpoch 149/150\n4/4 [==============================] - 0s 14ms/step - loss: 1.4538 - auc: 0.7847 - val_loss: 1.4305 - val_auc: 0.7868\nEpoch 150/150\n4/4 [==============================] - 0s 6ms/step - loss: 1.4529 - auc: 0.7858 - val_loss: 1.4322 - val_auc: 0.7859\n","output_type":"stream"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<keras.callbacks.History at 0x7f2aa831bdf0>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/31fab286-754b-4aa1-b000-177627ab0259","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"be4537aa","execution_start":1727701319577,"execution_millis":102,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"3393cd21b08a404381209790d313aba2","deepnote_cell_type":"code"},"source":"\nfrom sklearn.metrics import confusion_matrix\n# 1. Get predictions (probabilities) from the model\ny_pred = model.predict(X_test)\n\n# 2. Convert predicted probabilities to class labels\ny_pred_classes = np.argmax(y_pred, axis=1)  # Get the index of the highest probability for each sample\n\n# 3. Convert true labels (y_test) from one-hot encoded to class labels\ny_test_classes = np.argmax(y_test, axis=1)\n\n# 4. Now, compute the confusion matrix\ncm = confusion_matrix(y_test_classes, y_pred_classes)\n\nprint(cm)\n","block_group":"13ac0055bba544fc8ffc9e65430a6cfe","execution_count":5,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 0s 1ms/step\n[[ 0  0  0  0  0  3]\n [ 0  0  0  0  0  4]\n [ 0  0  0  0  0 14]\n [ 0  0  0  0  0 14]\n [ 0  0  0  0  0 26]\n [ 0  0  0  0  0 39]]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/15009857-d6be-488a-b753-d0e603c41430","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"4b11f2ccbb01406a9496535b5eaaa6e2","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"5ab255030fb84849a15d6aa5bbba0f0b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"e18b0f24587240fa8cf116a9e9764dd2","deepnote_cell_type":"text-cell-h3"},"source":"### Eerste Run","block_group":"605abe64c67940a7a6b164a56602680a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"53ea1125551d4855a04ff92af0651180","deepnote_cell_type":"text-cell-p"},"source":"\nmodel.compile(Adam(learning_rate=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\r\nmodel.fit(X_train, y_train, batch_size=100, epochs=15, validation_split=0.1)","block_group":"dc1676ade0d34456aa754189056a9624"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"4e61a42289bd4546bcc49af641b0e28c","deepnote_cell_type":"text-cell-p"},"source":"15 epochs 864ms","block_group":"bfbbd10eb19e4b26a88853b486495765"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"71b893efb83d483b9b95953a805dc759","deepnote_cell_type":"text-cell-p"},"source":"De confusion matrix zet alles in klasse 6","block_group":"414676660aae4a29891107f03301b821"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"11183723e98e4950b02490f3a1fa0402","deepnote_cell_type":"text-cell-p"},"source":"Conclusie: loss nog vrij hoog accuracy nog vrij laag, dus we zouden hem graag wat langer willen laten trainen, en de confusion matrix liet zien dat de accuracy hoogst was wanneer alles in klasse 6 werdt gezet. Dit betekend dat de model evaluation verkeerd niet alleen op accuracy zou moeten.","block_group":"d8bd200157c346d99f721fc964aabc58"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":10,"fromCodePoint":0}],"deepnote_app_block_visible":true,"cell_id":"1aadd45bf7ab453482ffc9f5b592cc15","deepnote_cell_type":"text-cell-p"},"source":"Tweede Run","block_group":"c8e165ea375c4ae2a39361c641d54479"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"2c9d11d7613246929cb56479101443c5","deepnote_cell_type":"text-cell-p"},"source":"model.compile(Adam(learning_rate=0.05), loss='categorical_crossentropy', metrics=['accuracy', AUC()])\r\nmodel.fit(X_train, y_train, batch_size=100, epochs=150, validation_split=0.1)","block_group":"461ee028a0984e65b97687995b6a32db"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"2835f25a8afc43f59248d8e2725814f7","deepnote_cell_type":"text-cell-p"},"source":"5s 929ms","block_group":"8821a1620e814776a89e0e1c5cb07cfb"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"766c7fe698ef4050bb1eb84ac352574f","deepnote_cell_type":"text-cell-p"},"source":"De confusion matrix voorspelt in de laatste 3 klassen 1 klas omhoog en omlaag","block_group":"1defcbef1dc04d638f1dc31b5d315b09"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"0096637dde3349faaa4761d0652279f9","deepnote_cell_type":"text-cell-p"},"source":"Vanaf epoch 110 ongeveer stabiel blijft","block_group":"cbd8823fc2f04295afe49aa325656930"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"d55a3ec5c790436fa4df4fb4670c2708","deepnote_cell_type":"text-cell-p"},"source":"Conclusie: Hij doet het al beter dan de eerste, dus de evaluation metrics zijn vrij goed. vanaf epoch 110 blijft hij vrij stabiel alleen springt de accuracy de hele tijd een beetje omhoog en omlaag, dus wij dachten dat de learning rate verlagen dit misschien wel op zou lossen.","block_group":"3897419fe1df44baaea7dda77f080a49"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":9,"fromCodePoint":0}],"deepnote_app_block_visible":true,"cell_id":"a89e2b76218e49dc9af62dc4c2a7c441","deepnote_cell_type":"text-cell-p"},"source":"Derde Run","block_group":"b72c6fbf40a84d989d8a1ba80d499d01"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"fffd2f8b126240cfadf73ebfe92a8337","deepnote_cell_type":"text-cell-p"},"source":"model.compile(Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=[AUC()])\r\nmodel.fit(X_train, y_train, batch_size=100, epochs=150, validation_split=0.1)","block_group":"4504a9c351b1459395d08ec5c5ff7801"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"5bbb643f1e8c43189cfd3012ce5c90c1","deepnote_cell_type":"text-cell-p"},"source":"5s 902ms (vrij snel nog)","block_group":"6785ebf428d5429c9894f32fc203a523"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"8ad709ff9990425da8fa432b503bd450","deepnote_cell_type":"text-cell-p"},"source":"De confusion matrix ziet er niet heel slecht uit, sommige zet hij nog wel verkeerd","block_group":"3f4b02e2739e4ea6a83b9466ee13ec81"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3761092d95aa4b5d98efdbcbcf370cd1","deepnote_cell_type":"text-cell-p"},"source":"[[ 3  0  0  0  0  0]\r\n [ 0  4  0  0  0  0]\r\n [ 0  3  9  1  1  0]\r\n [ 0  0  2  6  6  0]\r\n [ 0  0  0  4 19  3]\r\n [ 0  0  0  0  6 33]]","block_group":"eb82fbae586b459d988d37570a91b32b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"e07c7009b3a6411c96218ffb93158652","deepnote_cell_type":"text-cell-p"},"source":"met een lage learning rate flatlined de AUC rond de 120 epochs dus meer dan het huidige hebben we niet nodig.","block_group":"7c7bea5c3a3947c68636bd51e05a7133"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"6c0b8c4edb154ba98c95016f44c059ce","deepnote_cell_type":"text-cell-p"},"source":"Conclusie: Dit zijn de waardes waarmee we het beste resultaat verkrijgen. We merken alleen wel dat de dataset heel erg naar de laatste klasse leunt waardoor het NN af en toe alles in klassen 6 wilt zetten.","block_group":"293d70c7e3a24d87a5bdbfde5e091db0"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"0d4892394635480d9f8dd4f272544f49","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"09f9600b2c6c41afb7b2881e14c244c1"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"42583f8795434c5dbb427940730db775","deepnote_cell_type":"text-cell-p"},"source":"Optuna:","block_group":"7bcf41f130c94280bd8804ef8e473ff5"},{"cell_type":"code","metadata":{"source_hash":"a5f06074","execution_start":1727701319737,"execution_millis":401884,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"dddd7896b42d4501bf9569cb7735cfcc","deepnote_cell_type":"code"},"source":"import optuna\n\n# Define optuna objective function\ndef objective(trial):\n    # Define the hyperparameters to optimize\n    num_layers = trial.suggest_int('num_layers', 1, 3)\n    num_neurons = trial.suggest_int('num_neurons', 8, 64)\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n    batch_size = trial.suggest_int('batch_size', 8, 32)\n\n    # Create the model\n    model = Sequential()\n    model.add(Dense(4, input_shape=(X_train.shape[1],), activation='relu'))\n\n    for i in range(num_layers):\n        model.add(Dense(num_neurons, activation='relu'))\n\n    model.add(Dense(6, activation='softmax'))\n\n    # Compile the model\n    model.compile(Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=[AUC()])\n\n    # Fit the model\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=50, validation_split=0.1)\n\n    # Evaluate the model\n    _, auc = model.evaluate(X_test, y_test, verbose=0)\n\n    return auc\n\n# Create a study object and optimize the objective function\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)","block_group":"efd7f18fe98b4e93ab7c89cf98062f96","execution_count":6,"outputs":[{"name":"stderr","text":"18/18 [==============================] - 0s 2ms/step - loss: 0.6241 - auc_91: 0.9569 - val_loss: 0.8960 - val_auc_91: 0.9243\nEpoch 16/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6240 - auc_91: 0.9563 - val_loss: 0.8216 - val_auc_91: 0.9332\nEpoch 17/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6110 - auc_91: 0.9593 - val_loss: 0.8800 - val_auc_91: 0.9280\nEpoch 18/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6215 - auc_91: 0.9570 - val_loss: 0.7172 - val_auc_91: 0.9443\nEpoch 19/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5976 - auc_91: 0.9584 - val_loss: 0.7442 - val_auc_91: 0.9352\nEpoch 20/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6235 - auc_91: 0.9562 - val_loss: 0.7256 - val_auc_91: 0.9423\nEpoch 21/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.6227 - auc_91: 0.9568 - val_loss: 0.8689 - val_auc_91: 0.9277\nEpoch 22/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6134 - auc_91: 0.9590 - val_loss: 0.7216 - val_auc_91: 0.9372\nEpoch 23/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5845 - auc_91: 0.9618 - val_loss: 0.7885 - val_auc_91: 0.9334\nEpoch 24/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.6240 - auc_91: 0.9568 - val_loss: 0.8938 - val_auc_91: 0.9259\nEpoch 25/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5577 - auc_91: 0.9646 - val_loss: 0.8672 - val_auc_91: 0.9286\nEpoch 26/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.5501 - auc_91: 0.9655 - val_loss: 1.0663 - val_auc_91: 0.9138\nEpoch 27/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6495 - auc_91: 0.9544 - val_loss: 1.0829 - val_auc_91: 0.9106\nEpoch 28/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5906 - auc_91: 0.9607 - val_loss: 0.7607 - val_auc_91: 0.9344\nEpoch 29/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5653 - auc_91: 0.9634 - val_loss: 0.7121 - val_auc_91: 0.9414\nEpoch 30/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5445 - auc_91: 0.9655 - val_loss: 0.7616 - val_auc_91: 0.9351\nEpoch 31/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5849 - auc_91: 0.9605 - val_loss: 0.8444 - val_auc_91: 0.9256\nEpoch 32/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5652 - auc_91: 0.9640 - val_loss: 0.7413 - val_auc_91: 0.9358\nEpoch 33/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5492 - auc_91: 0.9653 - val_loss: 0.7186 - val_auc_91: 0.9336\nEpoch 34/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.5714 - auc_91: 0.9627 - val_loss: 0.7085 - val_auc_91: 0.9439\nEpoch 35/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5644 - auc_91: 0.9636 - val_loss: 0.8323 - val_auc_91: 0.9318\nEpoch 36/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5645 - auc_91: 0.9640 - val_loss: 0.7271 - val_auc_91: 0.9404\nEpoch 37/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5579 - auc_91: 0.9638 - val_loss: 0.7511 - val_auc_91: 0.9411\nEpoch 38/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.5972 - auc_91: 0.9593 - val_loss: 0.7833 - val_auc_91: 0.9365\nEpoch 39/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.6016 - auc_91: 0.9590 - val_loss: 1.0581 - val_auc_91: 0.9232\nEpoch 40/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5780 - auc_91: 0.9618 - val_loss: 0.7930 - val_auc_91: 0.9324\nEpoch 41/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5327 - auc_91: 0.9667 - val_loss: 0.7349 - val_auc_91: 0.9422\nEpoch 42/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5474 - auc_91: 0.9648 - val_loss: 0.9148 - val_auc_91: 0.9217\nEpoch 43/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.5785 - auc_91: 0.9616 - val_loss: 0.7865 - val_auc_91: 0.9341\nEpoch 44/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5849 - auc_91: 0.9615 - val_loss: 0.6625 - val_auc_91: 0.9457\nEpoch 45/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5650 - auc_91: 0.9636 - val_loss: 0.7551 - val_auc_91: 0.9384\nEpoch 46/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5634 - auc_91: 0.9639 - val_loss: 0.8560 - val_auc_91: 0.9252\nEpoch 47/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5840 - auc_91: 0.9599 - val_loss: 0.7810 - val_auc_91: 0.9303\nEpoch 48/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5628 - auc_91: 0.9633 - val_loss: 0.8874 - val_auc_91: 0.9267\nEpoch 49/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.5461 - auc_91: 0.9656 - val_loss: 0.7861 - val_auc_91: 0.9318\nEpoch 50/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5398 - auc_91: 0.9665 - val_loss: 0.7023 - val_auc_91: 0.9479\n[I 2024-09-30 13:08:07,357] Trial 90 finished with value: 0.9584900736808777 and parameters: {'num_layers': 3, 'num_neurons': 47, 'learning_rate': 0.0011655235871057537, 'batch_size': 20}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n14/14 [==============================] - 1s 17ms/step - loss: 2.7802 - auc_92: 0.7300 - val_loss: 1.1279 - val_auc_92: 0.8794\nEpoch 2/50\n14/14 [==============================] - 0s 2ms/step - loss: 1.0606 - auc_92: 0.8904 - val_loss: 1.0172 - val_auc_92: 0.8976\nEpoch 3/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.8507 - auc_92: 0.9274 - val_loss: 0.8342 - val_auc_92: 0.9228\nEpoch 4/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.7695 - auc_92: 0.9344 - val_loss: 0.7504 - val_auc_92: 0.9387\nEpoch 5/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6713 - auc_92: 0.9502 - val_loss: 0.7856 - val_auc_92: 0.9324\nEpoch 6/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6661 - auc_92: 0.9520 - val_loss: 0.6885 - val_auc_92: 0.9404\nEpoch 7/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6332 - auc_92: 0.9543 - val_loss: 0.7862 - val_auc_92: 0.9324\nEpoch 8/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.7510 - auc_92: 0.9418 - val_loss: 0.9273 - val_auc_92: 0.9131\nEpoch 9/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6948 - auc_92: 0.9489 - val_loss: 0.6873 - val_auc_92: 0.9494\nEpoch 10/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6492 - auc_92: 0.9536 - val_loss: 0.9787 - val_auc_92: 0.9121\nEpoch 11/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6672 - auc_92: 0.9519 - val_loss: 0.8198 - val_auc_92: 0.9291\nEpoch 12/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6225 - auc_92: 0.9571 - val_loss: 0.8368 - val_auc_92: 0.9306\nEpoch 13/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6410 - auc_92: 0.9555 - val_loss: 0.6811 - val_auc_92: 0.9443\nEpoch 14/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5954 - auc_92: 0.9605 - val_loss: 0.7539 - val_auc_92: 0.9368\nEpoch 15/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.5643 - auc_92: 0.9652 - val_loss: 0.7506 - val_auc_92: 0.9426\nEpoch 16/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6020 - auc_92: 0.9602 - val_loss: 0.6230 - val_auc_92: 0.9553\nEpoch 17/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5546 - auc_92: 0.9650 - val_loss: 0.7682 - val_auc_92: 0.9345\nEpoch 18/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5708 - auc_92: 0.9626 - val_loss: 0.7080 - val_auc_92: 0.9401\nEpoch 19/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5807 - auc_92: 0.9622 - val_loss: 0.7897 - val_auc_92: 0.9311\nEpoch 20/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5449 - auc_92: 0.9669 - val_loss: 0.7380 - val_auc_92: 0.9421\nEpoch 21/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6106 - auc_92: 0.9592 - val_loss: 0.6691 - val_auc_92: 0.9499\nEpoch 22/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.5545 - auc_92: 0.9647 - val_loss: 0.8794 - val_auc_92: 0.9249\nEpoch 23/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5458 - auc_92: 0.9663 - val_loss: 0.8590 - val_auc_92: 0.9295\nEpoch 24/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5536 - auc_92: 0.9650 - val_loss: 0.8185 - val_auc_92: 0.9303\nEpoch 25/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5593 - auc_92: 0.9645 - val_loss: 0.7745 - val_auc_92: 0.9396\nEpoch 26/50\n14/14 [==============================] - 0s 6ms/step - loss: 0.5402 - auc_92: 0.9671 - val_loss: 0.7350 - val_auc_92: 0.9421\nEpoch 27/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5459 - auc_92: 0.9652 - val_loss: 0.6533 - val_auc_92: 0.9501\nEpoch 28/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5459 - auc_92: 0.9664 - val_loss: 0.9631 - val_auc_92: 0.9171\nEpoch 29/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5930 - auc_92: 0.9615 - val_loss: 0.8589 - val_auc_92: 0.9188\nEpoch 30/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5659 - auc_92: 0.9632 - val_loss: 0.7321 - val_auc_92: 0.9427\nEpoch 31/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.5523 - auc_92: 0.9657 - val_loss: 0.6867 - val_auc_92: 0.9477\nEpoch 32/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6003 - auc_92: 0.9586 - val_loss: 0.8026 - val_auc_92: 0.9343\nEpoch 33/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6202 - auc_92: 0.9565 - val_loss: 0.8178 - val_auc_92: 0.9316\nEpoch 34/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5444 - auc_92: 0.9665 - val_loss: 0.7638 - val_auc_92: 0.9371\nEpoch 35/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5443 - auc_92: 0.9650 - val_loss: 0.7455 - val_auc_92: 0.9381\nEpoch 36/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.5359 - auc_92: 0.9666 - val_loss: 0.7326 - val_auc_92: 0.9419\nEpoch 37/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.5812 - auc_92: 0.9614 - val_loss: 0.7435 - val_auc_92: 0.9413\nEpoch 38/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5705 - auc_92: 0.9632 - val_loss: 0.7618 - val_auc_92: 0.9358\nEpoch 39/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5363 - auc_92: 0.9675 - val_loss: 0.8061 - val_auc_92: 0.9374\nEpoch 40/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.5586 - auc_92: 0.9652 - val_loss: 0.7528 - val_auc_92: 0.9433\nEpoch 41/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5820 - auc_92: 0.9622 - val_loss: 0.8691 - val_auc_92: 0.9234\nEpoch 42/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.5811 - auc_92: 0.9614 - val_loss: 0.6510 - val_auc_92: 0.9544\nEpoch 43/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5239 - auc_92: 0.9690 - val_loss: 0.6539 - val_auc_92: 0.9482\nEpoch 44/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5185 - auc_92: 0.9691 - val_loss: 0.6582 - val_auc_92: 0.9547\nEpoch 45/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.4988 - auc_92: 0.9723 - val_loss: 0.7720 - val_auc_92: 0.9386\nEpoch 46/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5148 - auc_92: 0.9697 - val_loss: 0.8945 - val_auc_92: 0.9227\nEpoch 47/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6018 - auc_92: 0.9608 - val_loss: 0.6945 - val_auc_92: 0.9431\nEpoch 48/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5777 - auc_92: 0.9630 - val_loss: 0.7810 - val_auc_92: 0.9356\nEpoch 49/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.5300 - auc_92: 0.9684 - val_loss: 0.7445 - val_auc_92: 0.9368\nEpoch 50/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.4875 - auc_92: 0.9732 - val_loss: 0.6367 - val_auc_92: 0.9529\n[I 2024-09-30 13:08:10,736] Trial 91 finished with value: 0.9637699723243713 and parameters: {'num_layers': 3, 'num_neurons': 25, 'learning_rate': 0.009890555668954158, 'batch_size': 26}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n15/15 [==============================] - 1s 15ms/step - loss: 3.9289 - auc_93: 0.6523 - val_loss: 1.4135 - val_auc_93: 0.8257\nEpoch 2/50\n15/15 [==============================] - 0s 4ms/step - loss: 1.2676 - auc_93: 0.8443 - val_loss: 1.1015 - val_auc_93: 0.8769\nEpoch 3/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.9522 - auc_93: 0.9112 - val_loss: 1.0205 - val_auc_93: 0.8941\nEpoch 4/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.8445 - auc_93: 0.9281 - val_loss: 0.9460 - val_auc_93: 0.9064\nEpoch 5/50\n15/15 [==============================] - 0s 3ms/step - loss: 0.8320 - auc_93: 0.9273 - val_loss: 0.8513 - val_auc_93: 0.9155\nEpoch 6/50\n15/15 [==============================] - 0s 3ms/step - loss: 0.7480 - auc_93: 0.9401 - val_loss: 0.8606 - val_auc_93: 0.9162\nEpoch 7/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.7266 - auc_93: 0.9438 - val_loss: 0.9457 - val_auc_93: 0.9030\nEpoch 8/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.8529 - auc_93: 0.9248 - val_loss: 0.7642 - val_auc_93: 0.9287\nEpoch 9/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.7079 - auc_93: 0.9456 - val_loss: 0.7183 - val_auc_93: 0.9371\nEpoch 10/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.7417 - auc_93: 0.9431 - val_loss: 0.8126 - val_auc_93: 0.9264\nEpoch 11/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.7154 - auc_93: 0.9442 - val_loss: 0.6830 - val_auc_93: 0.9378\nEpoch 12/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.7427 - auc_93: 0.9396 - val_loss: 0.7416 - val_auc_93: 0.9342\nEpoch 13/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.7533 - auc_93: 0.9352 - val_loss: 0.7992 - val_auc_93: 0.9286\nEpoch 14/50\n15/15 [==============================] - 0s 3ms/step - loss: 0.6896 - auc_93: 0.9475 - val_loss: 0.8461 - val_auc_93: 0.9192\nEpoch 15/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.6197 - auc_93: 0.9581 - val_loss: 0.8980 - val_auc_93: 0.9154\nEpoch 16/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.6274 - auc_93: 0.9546 - val_loss: 0.7932 - val_auc_93: 0.9292\nEpoch 17/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.6469 - auc_93: 0.9508 - val_loss: 0.7144 - val_auc_93: 0.9401\nEpoch 18/50\n15/15 [==============================] - 0s 3ms/step - loss: 0.6667 - auc_93: 0.9504 - val_loss: 0.7069 - val_auc_93: 0.9395\nEpoch 19/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.7842 - auc_93: 0.9359 - val_loss: 0.6770 - val_auc_93: 0.9468\nEpoch 20/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.6638 - auc_93: 0.9493 - val_loss: 0.7216 - val_auc_93: 0.9314\nEpoch 21/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.6474 - auc_93: 0.9524 - val_loss: 0.8175 - val_auc_93: 0.9223\nEpoch 22/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5702 - auc_93: 0.9627 - val_loss: 0.7141 - val_auc_93: 0.9434\nEpoch 23/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5906 - auc_93: 0.9608 - val_loss: 0.6860 - val_auc_93: 0.9445\nEpoch 24/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.6026 - auc_93: 0.9595 - val_loss: 0.6999 - val_auc_93: 0.9412\nEpoch 25/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5953 - auc_93: 0.9588 - val_loss: 0.7534 - val_auc_93: 0.9373\nEpoch 26/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.5955 - auc_93: 0.9592 - val_loss: 0.8261 - val_auc_93: 0.9271\nEpoch 27/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.6379 - auc_93: 0.9524 - val_loss: 0.6961 - val_auc_93: 0.9369\nEpoch 28/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.8311 - auc_93: 0.9355 - val_loss: 0.7618 - val_auc_93: 0.9356\nEpoch 29/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.7145 - auc_93: 0.9451 - val_loss: 0.8515 - val_auc_93: 0.9178\nEpoch 30/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.6107 - auc_93: 0.9596 - val_loss: 0.8445 - val_auc_93: 0.9231\nEpoch 31/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5782 - auc_93: 0.9621 - val_loss: 0.7733 - val_auc_93: 0.9333\nEpoch 32/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.5405 - auc_93: 0.9678 - val_loss: 0.8319 - val_auc_93: 0.9296\nEpoch 33/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.5303 - auc_93: 0.9674 - val_loss: 0.7712 - val_auc_93: 0.9321\nEpoch 34/50\n15/15 [==============================] - 0s 3ms/step - loss: 0.5835 - auc_93: 0.9611 - val_loss: 0.6636 - val_auc_93: 0.9515\nEpoch 35/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.6212 - auc_93: 0.9565 - val_loss: 0.6673 - val_auc_93: 0.9443\nEpoch 36/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.6556 - auc_93: 0.9531 - val_loss: 0.7205 - val_auc_93: 0.9419\nEpoch 37/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.6317 - auc_93: 0.9546 - val_loss: 0.9247 - val_auc_93: 0.9097\nEpoch 38/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.6111 - auc_93: 0.9571 - val_loss: 0.6982 - val_auc_93: 0.9413\nEpoch 39/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.5487 - auc_93: 0.9657 - val_loss: 0.9950 - val_auc_93: 0.9117\nEpoch 40/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5936 - auc_93: 0.9605 - val_loss: 0.8110 - val_auc_93: 0.9288\nEpoch 41/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.5577 - auc_93: 0.9646 - val_loss: 0.7215 - val_auc_93: 0.9423\nEpoch 42/50\n15/15 [==============================] - 0s 3ms/step - loss: 0.6037 - auc_93: 0.9599 - val_loss: 0.6461 - val_auc_93: 0.9547\nEpoch 43/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.5933 - auc_93: 0.9592 - val_loss: 0.7541 - val_auc_93: 0.9383\nEpoch 44/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5781 - auc_93: 0.9616 - val_loss: 0.8882 - val_auc_93: 0.9195\nEpoch 45/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.5062 - auc_93: 0.9707 - val_loss: 0.7266 - val_auc_93: 0.9394\nEpoch 46/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5259 - auc_93: 0.9680 - val_loss: 0.7777 - val_auc_93: 0.9382\nEpoch 47/50\n15/15 [==============================] - 0s 5ms/step - loss: 0.5101 - auc_93: 0.9702 - val_loss: 0.6699 - val_auc_93: 0.9457\nEpoch 48/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5323 - auc_93: 0.9674 - val_loss: 0.9147 - val_auc_93: 0.9129\nEpoch 49/50\n15/15 [==============================] - 0s 4ms/step - loss: 0.5447 - auc_93: 0.9650 - val_loss: 0.8993 - val_auc_93: 0.9258\nEpoch 50/50\n15/15 [==============================] - 0s 2ms/step - loss: 0.5540 - auc_93: 0.9640 - val_loss: 0.7432 - val_auc_93: 0.9374\n[I 2024-09-30 13:08:14,106] Trial 92 finished with value: 0.9546698927879333 and parameters: {'num_layers': 3, 'num_neurons': 39, 'learning_rate': 0.00799489171474586, 'batch_size': 24}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n13/13 [==============================] - 1s 17ms/step - loss: 3.7425 - auc_94: 0.6624 - val_loss: 2.3040 - val_auc_94: 0.6532\nEpoch 2/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.7520 - auc_94: 0.7076 - val_loss: 1.6697 - val_auc_94: 0.6931\nEpoch 3/50\n13/13 [==============================] - 0s 2ms/step - loss: 1.3821 - auc_94: 0.7908 - val_loss: 1.2292 - val_auc_94: 0.8466\nEpoch 4/50\n13/13 [==============================] - 0s 5ms/step - loss: 1.1881 - auc_94: 0.8524 - val_loss: 1.0802 - val_auc_94: 0.8863\nEpoch 5/50\n13/13 [==============================] - 0s 2ms/step - loss: 1.0320 - auc_94: 0.9000 - val_loss: 1.0055 - val_auc_94: 0.9087\nEpoch 6/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.9287 - auc_94: 0.9138 - val_loss: 1.0294 - val_auc_94: 0.8894\nEpoch 7/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.8585 - auc_94: 0.9252 - val_loss: 0.8867 - val_auc_94: 0.9154\nEpoch 8/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.8279 - auc_94: 0.9287 - val_loss: 0.9021 - val_auc_94: 0.9103\nEpoch 9/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7764 - auc_94: 0.9380 - val_loss: 0.9158 - val_auc_94: 0.9098\nEpoch 10/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7475 - auc_94: 0.9415 - val_loss: 0.9070 - val_auc_94: 0.9169\nEpoch 11/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.7192 - auc_94: 0.9461 - val_loss: 0.8428 - val_auc_94: 0.9286\nEpoch 12/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7832 - auc_94: 0.9352 - val_loss: 0.9057 - val_auc_94: 0.9120\nEpoch 13/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7416 - auc_94: 0.9409 - val_loss: 0.9301 - val_auc_94: 0.9114\nEpoch 14/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.7939 - auc_94: 0.9324 - val_loss: 0.9580 - val_auc_94: 0.9154\nEpoch 15/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7229 - auc_94: 0.9442 - val_loss: 1.0596 - val_auc_94: 0.8971\nEpoch 16/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7531 - auc_94: 0.9368 - val_loss: 0.9775 - val_auc_94: 0.8978\nEpoch 17/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7280 - auc_94: 0.9430 - val_loss: 0.8833 - val_auc_94: 0.9171\nEpoch 18/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.6558 - auc_94: 0.9536 - val_loss: 0.7636 - val_auc_94: 0.9293\nEpoch 19/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6900 - auc_94: 0.9481 - val_loss: 0.8704 - val_auc_94: 0.9172\nEpoch 20/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6780 - auc_94: 0.9495 - val_loss: 0.9648 - val_auc_94: 0.9072\nEpoch 21/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6524 - auc_94: 0.9524 - val_loss: 0.7231 - val_auc_94: 0.9418\nEpoch 22/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6259 - auc_94: 0.9573 - val_loss: 0.8698 - val_auc_94: 0.9279\nEpoch 23/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.6891 - auc_94: 0.9472 - val_loss: 1.0503 - val_auc_94: 0.8939\nEpoch 24/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.7090 - auc_94: 0.9448 - val_loss: 0.6913 - val_auc_94: 0.9429\nEpoch 25/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7116 - auc_94: 0.9448 - val_loss: 0.8154 - val_auc_94: 0.9324\nEpoch 26/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.7201 - auc_94: 0.9437 - val_loss: 1.0228 - val_auc_94: 0.8996\nEpoch 27/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6531 - auc_94: 0.9518 - val_loss: 0.8573 - val_auc_94: 0.9205\nEpoch 28/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6328 - auc_94: 0.9564 - val_loss: 0.9050 - val_auc_94: 0.9292\nEpoch 29/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6845 - auc_94: 0.9492 - val_loss: 0.8849 - val_auc_94: 0.9195\nEpoch 30/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5755 - auc_94: 0.9638 - val_loss: 0.7518 - val_auc_94: 0.9337\nEpoch 31/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.5809 - auc_94: 0.9617 - val_loss: 0.7444 - val_auc_94: 0.9348\nEpoch 32/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5593 - auc_94: 0.9650 - val_loss: 0.6781 - val_auc_94: 0.9449\nEpoch 33/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5561 - auc_94: 0.9641 - val_loss: 0.7339 - val_auc_94: 0.9423\nEpoch 34/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5761 - auc_94: 0.9627 - val_loss: 0.8712 - val_auc_94: 0.9198\nEpoch 35/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.6078 - auc_94: 0.9581 - val_loss: 0.8375 - val_auc_94: 0.9209\nEpoch 36/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7668 - auc_94: 0.9401 - val_loss: 0.8289 - val_auc_94: 0.9254\nEpoch 37/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.6538 - auc_94: 0.9526 - val_loss: 0.7848 - val_auc_94: 0.9327\nEpoch 38/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5446 - auc_94: 0.9674 - val_loss: 0.8588 - val_auc_94: 0.9244\nEpoch 39/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5866 - auc_94: 0.9617 - val_loss: 0.8777 - val_auc_94: 0.9259\nEpoch 40/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5430 - auc_94: 0.9666 - val_loss: 0.8370 - val_auc_94: 0.9292\nEpoch 41/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5650 - auc_94: 0.9639 - val_loss: 0.7665 - val_auc_94: 0.9344\nEpoch 42/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5426 - auc_94: 0.9662 - val_loss: 0.7080 - val_auc_94: 0.9421\nEpoch 43/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5732 - auc_94: 0.9625 - val_loss: 0.7277 - val_auc_94: 0.9425\nEpoch 44/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6283 - auc_94: 0.9553 - val_loss: 0.6517 - val_auc_94: 0.9469\nEpoch 45/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5641 - auc_94: 0.9650 - val_loss: 0.8087 - val_auc_94: 0.9289\nEpoch 46/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.6127 - auc_94: 0.9567 - val_loss: 0.8541 - val_auc_94: 0.9265\nEpoch 47/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5916 - auc_94: 0.9611 - val_loss: 1.1688 - val_auc_94: 0.8844\nEpoch 48/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6114 - auc_94: 0.9581 - val_loss: 0.7116 - val_auc_94: 0.9402\nEpoch 49/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6661 - auc_94: 0.9494 - val_loss: 0.6777 - val_auc_94: 0.9489\nEpoch 50/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6076 - auc_94: 0.9589 - val_loss: 0.6369 - val_auc_94: 0.9534\n[I 2024-09-30 13:08:17,467] Trial 93 finished with value: 0.9637799263000488 and parameters: {'num_layers': 3, 'num_neurons': 35, 'learning_rate': 0.006419706050225067, 'batch_size': 29}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n18/18 [==============================] - 1s 39ms/step - loss: 6.6505 - auc_95: 0.5602 - val_loss: 3.4233 - val_auc_95: 0.6743\nEpoch 2/50\n18/18 [==============================] - 0s 5ms/step - loss: 2.3657 - auc_95: 0.7141 - val_loss: 1.6052 - val_auc_95: 0.7651\nEpoch 3/50\n18/18 [==============================] - 0s 4ms/step - loss: 1.5031 - auc_95: 0.7908 - val_loss: 1.4614 - val_auc_95: 0.7882\nEpoch 4/50\n18/18 [==============================] - 0s 4ms/step - loss: 1.5880 - auc_95: 0.7781 - val_loss: 1.3065 - val_auc_95: 0.8386\nEpoch 5/50\n18/18 [==============================] - 0s 3ms/step - loss: 1.4294 - auc_95: 0.8208 - val_loss: 1.7638 - val_auc_95: 0.7352\nEpoch 6/50\n18/18 [==============================] - 0s 5ms/step - loss: 1.2379 - auc_95: 0.8539 - val_loss: 1.1790 - val_auc_95: 0.8688\nEpoch 7/50\n18/18 [==============================] - 0s 4ms/step - loss: 1.0774 - auc_95: 0.8854 - val_loss: 0.9239 - val_auc_95: 0.9131\nEpoch 8/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.8799 - auc_95: 0.9231 - val_loss: 0.8765 - val_auc_95: 0.9156\nEpoch 9/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.8433 - auc_95: 0.9281 - val_loss: 0.9646 - val_auc_95: 0.9052\nEpoch 10/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.7969 - auc_95: 0.9352 - val_loss: 1.2329 - val_auc_95: 0.9099\nEpoch 11/50\n18/18 [==============================] - 0s 4ms/step - loss: 1.0111 - auc_95: 0.9098 - val_loss: 1.4969 - val_auc_95: 0.8481\nEpoch 12/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.7958 - auc_95: 0.9339 - val_loss: 1.0254 - val_auc_95: 0.9018\nEpoch 13/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.7425 - auc_95: 0.9424 - val_loss: 0.8219 - val_auc_95: 0.9266\nEpoch 14/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.6910 - auc_95: 0.9479 - val_loss: 0.7289 - val_auc_95: 0.9385\nEpoch 15/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6877 - auc_95: 0.9486 - val_loss: 0.9338 - val_auc_95: 0.9226\nEpoch 16/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.7190 - auc_95: 0.9449 - val_loss: 1.0787 - val_auc_95: 0.9011\nEpoch 17/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.7356 - auc_95: 0.9426 - val_loss: 0.9235 - val_auc_95: 0.9185\nEpoch 18/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6376 - auc_95: 0.9552 - val_loss: 0.7317 - val_auc_95: 0.9432\nEpoch 19/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.6580 - auc_95: 0.9519 - val_loss: 0.8282 - val_auc_95: 0.9314\nEpoch 20/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6714 - auc_95: 0.9494 - val_loss: 1.1337 - val_auc_95: 0.9034\nEpoch 21/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.7319 - auc_95: 0.9435 - val_loss: 0.8419 - val_auc_95: 0.9355\nEpoch 22/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.6868 - auc_95: 0.9478 - val_loss: 0.7871 - val_auc_95: 0.9311\nEpoch 23/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6507 - auc_95: 0.9522 - val_loss: 0.9532 - val_auc_95: 0.9151\nEpoch 24/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.8835 - auc_95: 0.9292 - val_loss: 0.8050 - val_auc_95: 0.9364\nEpoch 25/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.8014 - auc_95: 0.9363 - val_loss: 0.8666 - val_auc_95: 0.9262\nEpoch 26/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6907 - auc_95: 0.9495 - val_loss: 0.8207 - val_auc_95: 0.9274\nEpoch 27/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6134 - auc_95: 0.9578 - val_loss: 0.8975 - val_auc_95: 0.9227\nEpoch 28/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6896 - auc_95: 0.9480 - val_loss: 1.0321 - val_auc_95: 0.9046\nEpoch 29/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6147 - auc_95: 0.9572 - val_loss: 0.7369 - val_auc_95: 0.9352\nEpoch 30/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6214 - auc_95: 0.9573 - val_loss: 0.7852 - val_auc_95: 0.9341\nEpoch 31/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.6306 - auc_95: 0.9554 - val_loss: 0.8924 - val_auc_95: 0.9212\nEpoch 32/50\n18/18 [==============================] - 0s 6ms/step - loss: 0.6126 - auc_95: 0.9586 - val_loss: 0.7150 - val_auc_95: 0.9371\nEpoch 33/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.5878 - auc_95: 0.9595 - val_loss: 0.7778 - val_auc_95: 0.9379\nEpoch 34/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6970 - auc_95: 0.9469 - val_loss: 0.7289 - val_auc_95: 0.9400\nEpoch 35/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6266 - auc_95: 0.9553 - val_loss: 0.7541 - val_auc_95: 0.9311\nEpoch 36/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.7110 - auc_95: 0.9433 - val_loss: 0.8047 - val_auc_95: 0.9407\nEpoch 37/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.6006 - auc_95: 0.9580 - val_loss: 0.7454 - val_auc_95: 0.9386\nEpoch 38/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5695 - auc_95: 0.9641 - val_loss: 0.7254 - val_auc_95: 0.9437\nEpoch 39/50\n18/18 [==============================] - 0s 3ms/step - loss: 0.6210 - auc_95: 0.9567 - val_loss: 0.7618 - val_auc_95: 0.9334\nEpoch 40/50\n18/18 [==============================] - 0s 5ms/step - loss: 0.5808 - auc_95: 0.9616 - val_loss: 0.6845 - val_auc_95: 0.9479\nEpoch 41/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6230 - auc_95: 0.9556 - val_loss: 0.7541 - val_auc_95: 0.9406\nEpoch 42/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5707 - auc_95: 0.9632 - val_loss: 0.6729 - val_auc_95: 0.9437\nEpoch 43/50\n18/18 [==============================] - 0s 6ms/step - loss: 0.6251 - auc_95: 0.9567 - val_loss: 0.6480 - val_auc_95: 0.9546\nEpoch 44/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5696 - auc_95: 0.9631 - val_loss: 0.7581 - val_auc_95: 0.9462\nEpoch 45/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5878 - auc_95: 0.9601 - val_loss: 0.7698 - val_auc_95: 0.9369\nEpoch 46/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.5646 - auc_95: 0.9637 - val_loss: 0.7825 - val_auc_95: 0.9314\nEpoch 47/50\n18/18 [==============================] - 0s 6ms/step - loss: 0.5526 - auc_95: 0.9646 - val_loss: 0.8212 - val_auc_95: 0.9308\nEpoch 48/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.5961 - auc_95: 0.9596 - val_loss: 1.0056 - val_auc_95: 0.9029\nEpoch 49/50\n18/18 [==============================] - 0s 2ms/step - loss: 0.6007 - auc_95: 0.9578 - val_loss: 0.9654 - val_auc_95: 0.9137\nEpoch 50/50\n18/18 [==============================] - 0s 4ms/step - loss: 0.6483 - auc_95: 0.9532 - val_loss: 0.8349 - val_auc_95: 0.9276\n[I 2024-09-30 13:08:22,395] Trial 94 finished with value: 0.9430100321769714 and parameters: {'num_layers': 3, 'num_neurons': 37, 'learning_rate': 0.004607880145539893, 'batch_size': 21}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n33/33 [==============================] - 1s 9ms/step - loss: 1.7401 - auc_96: 0.7645 - val_loss: 0.9503 - val_auc_96: 0.9061\nEpoch 2/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.8311 - auc_96: 0.9304 - val_loss: 0.8300 - val_auc_96: 0.9255\nEpoch 3/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6604 - auc_96: 0.9517 - val_loss: 0.9880 - val_auc_96: 0.9005\nEpoch 4/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.7742 - auc_96: 0.9368 - val_loss: 1.0371 - val_auc_96: 0.8959\nEpoch 5/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.8169 - auc_96: 0.9253 - val_loss: 0.7526 - val_auc_96: 0.9377\nEpoch 6/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.7008 - auc_96: 0.9447 - val_loss: 0.7153 - val_auc_96: 0.9464\nEpoch 7/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.7696 - auc_96: 0.9381 - val_loss: 0.6769 - val_auc_96: 0.9446\nEpoch 8/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6786 - auc_96: 0.9470 - val_loss: 0.8716 - val_auc_96: 0.9204\nEpoch 9/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6789 - auc_96: 0.9496 - val_loss: 0.7591 - val_auc_96: 0.9329\nEpoch 10/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6353 - auc_96: 0.9541 - val_loss: 0.8939 - val_auc_96: 0.9161\nEpoch 11/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6972 - auc_96: 0.9467 - val_loss: 0.8138 - val_auc_96: 0.9334\nEpoch 12/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.6589 - auc_96: 0.9522 - val_loss: 0.7877 - val_auc_96: 0.9322\nEpoch 13/50\n33/33 [==============================] - 0s 4ms/step - loss: 0.6343 - auc_96: 0.9551 - val_loss: 0.8223 - val_auc_96: 0.9292\nEpoch 14/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - auc_96: 0.9494 - val_loss: 0.8397 - val_auc_96: 0.9333\nEpoch 15/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6806 - auc_96: 0.9482 - val_loss: 0.8240 - val_auc_96: 0.9233\nEpoch 16/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6817 - auc_96: 0.9485 - val_loss: 0.8267 - val_auc_96: 0.9287\nEpoch 17/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.7272 - auc_96: 0.9428 - val_loss: 0.7478 - val_auc_96: 0.9414\nEpoch 18/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6573 - auc_96: 0.9502 - val_loss: 0.7773 - val_auc_96: 0.9337\nEpoch 19/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6129 - auc_96: 0.9576 - val_loss: 0.8020 - val_auc_96: 0.9282\nEpoch 20/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6197 - auc_96: 0.9561 - val_loss: 1.0203 - val_auc_96: 0.9024\nEpoch 21/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6224 - auc_96: 0.9576 - val_loss: 0.7681 - val_auc_96: 0.9358\nEpoch 22/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5673 - auc_96: 0.9639 - val_loss: 0.8697 - val_auc_96: 0.9233\nEpoch 23/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6264 - auc_96: 0.9558 - val_loss: 0.7718 - val_auc_96: 0.9342\nEpoch 24/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6612 - auc_96: 0.9504 - val_loss: 0.7565 - val_auc_96: 0.9311\nEpoch 25/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5916 - auc_96: 0.9601 - val_loss: 0.6910 - val_auc_96: 0.9447\nEpoch 26/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6176 - auc_96: 0.9565 - val_loss: 0.7542 - val_auc_96: 0.9376\nEpoch 27/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6258 - auc_96: 0.9556 - val_loss: 1.1229 - val_auc_96: 0.8969\nEpoch 28/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6258 - auc_96: 0.9549 - val_loss: 0.7341 - val_auc_96: 0.9365\nEpoch 29/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.6680 - auc_96: 0.9502 - val_loss: 0.9258 - val_auc_96: 0.9229\nEpoch 30/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6512 - auc_96: 0.9505 - val_loss: 0.8531 - val_auc_96: 0.9249\nEpoch 31/50\n33/33 [==============================] - 0s 4ms/step - loss: 0.5863 - auc_96: 0.9610 - val_loss: 0.6845 - val_auc_96: 0.9437\nEpoch 32/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.5389 - auc_96: 0.9668 - val_loss: 0.7931 - val_auc_96: 0.9347\nEpoch 33/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5555 - auc_96: 0.9646 - val_loss: 0.8523 - val_auc_96: 0.9284\nEpoch 34/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6174 - auc_96: 0.9564 - val_loss: 0.6661 - val_auc_96: 0.9456\nEpoch 35/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5905 - auc_96: 0.9598 - val_loss: 1.0955 - val_auc_96: 0.8893\nEpoch 36/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5940 - auc_96: 0.9588 - val_loss: 0.7297 - val_auc_96: 0.9401\nEpoch 37/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5759 - auc_96: 0.9621 - val_loss: 0.8209 - val_auc_96: 0.9256\nEpoch 38/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6030 - auc_96: 0.9587 - val_loss: 0.7311 - val_auc_96: 0.9426\nEpoch 39/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5526 - auc_96: 0.9661 - val_loss: 0.8892 - val_auc_96: 0.9245\nEpoch 40/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5472 - auc_96: 0.9653 - val_loss: 0.8302 - val_auc_96: 0.9225\nEpoch 41/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6350 - auc_96: 0.9556 - val_loss: 0.9015 - val_auc_96: 0.9155\nEpoch 42/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6156 - auc_96: 0.9567 - val_loss: 0.7783 - val_auc_96: 0.9303\nEpoch 43/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5469 - auc_96: 0.9666 - val_loss: 0.7097 - val_auc_96: 0.9424\nEpoch 44/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.5709 - auc_96: 0.9625 - val_loss: 0.7479 - val_auc_96: 0.9389\nEpoch 45/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5443 - auc_96: 0.9660 - val_loss: 0.6266 - val_auc_96: 0.9542\nEpoch 46/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.6529 - auc_96: 0.9518 - val_loss: 0.6423 - val_auc_96: 0.9539\nEpoch 47/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5777 - auc_96: 0.9633 - val_loss: 0.6788 - val_auc_96: 0.9456\nEpoch 48/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.5671 - auc_96: 0.9626 - val_loss: 0.7410 - val_auc_96: 0.9338\nEpoch 49/50\n33/33 [==============================] - 0s 2ms/step - loss: 0.5963 - auc_96: 0.9607 - val_loss: 1.0164 - val_auc_96: 0.9067\nEpoch 50/50\n33/33 [==============================] - 0s 3ms/step - loss: 0.5590 - auc_96: 0.9639 - val_loss: 0.6610 - val_auc_96: 0.9398\n[I 2024-09-30 13:08:28,025] Trial 95 finished with value: 0.9628700017929077 and parameters: {'num_layers': 3, 'num_neurons': 31, 'learning_rate': 0.006901221759145143, 'batch_size': 11}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n14/14 [==============================] - 1s 17ms/step - loss: 1.1908 - auc_97: 0.8415 - val_loss: 1.0446 - val_auc_97: 0.8746\nEpoch 2/50\n14/14 [==============================] - 0s 4ms/step - loss: 1.0650 - auc_97: 0.8842 - val_loss: 0.9966 - val_auc_97: 0.9056\nEpoch 3/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.9416 - auc_97: 0.9103 - val_loss: 1.0681 - val_auc_97: 0.8771\nEpoch 4/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.8674 - auc_97: 0.9210 - val_loss: 1.2815 - val_auc_97: 0.8569\nEpoch 5/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.8379 - auc_97: 0.9287 - val_loss: 1.0717 - val_auc_97: 0.8904\nEpoch 6/50\n14/14 [==============================] - 0s 6ms/step - loss: 0.7934 - auc_97: 0.9339 - val_loss: 0.9925 - val_auc_97: 0.9014\nEpoch 7/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.7696 - auc_97: 0.9367 - val_loss: 1.0371 - val_auc_97: 0.8941\nEpoch 8/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.7342 - auc_97: 0.9418 - val_loss: 1.1026 - val_auc_97: 0.8849\nEpoch 9/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.7632 - auc_97: 0.9366 - val_loss: 0.9599 - val_auc_97: 0.9069\nEpoch 10/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.7240 - auc_97: 0.9427 - val_loss: 1.0782 - val_auc_97: 0.8884\nEpoch 11/50\n14/14 [==============================] - 0s 6ms/step - loss: 0.7334 - auc_97: 0.9405 - val_loss: 0.9575 - val_auc_97: 0.9002\nEpoch 12/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.7276 - auc_97: 0.9422 - val_loss: 1.0137 - val_auc_97: 0.8984\nEpoch 13/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.7491 - auc_97: 0.9395 - val_loss: 0.9051 - val_auc_97: 0.9074\nEpoch 14/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.7061 - auc_97: 0.9458 - val_loss: 0.8611 - val_auc_97: 0.9143\nEpoch 15/50\n14/14 [==============================] - 0s 7ms/step - loss: 0.6792 - auc_97: 0.9473 - val_loss: 0.7968 - val_auc_97: 0.9220\nEpoch 16/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6758 - auc_97: 0.9466 - val_loss: 0.8955 - val_auc_97: 0.9001\nEpoch 17/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6810 - auc_97: 0.9437 - val_loss: 0.7948 - val_auc_97: 0.9229\nEpoch 18/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6510 - auc_97: 0.9504 - val_loss: 0.7382 - val_auc_97: 0.9320\nEpoch 19/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6705 - auc_97: 0.9495 - val_loss: 0.6651 - val_auc_97: 0.9466\nEpoch 20/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6736 - auc_97: 0.9514 - val_loss: 0.7841 - val_auc_97: 0.9283\nEpoch 21/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6388 - auc_97: 0.9543 - val_loss: 0.8767 - val_auc_97: 0.9149\nEpoch 22/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6615 - auc_97: 0.9520 - val_loss: 0.7589 - val_auc_97: 0.9271\nEpoch 23/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6809 - auc_97: 0.9487 - val_loss: 0.7673 - val_auc_97: 0.9316\nEpoch 24/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6712 - auc_97: 0.9503 - val_loss: 0.8667 - val_auc_97: 0.9145\nEpoch 25/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6447 - auc_97: 0.9544 - val_loss: 0.8137 - val_auc_97: 0.9249\nEpoch 26/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6709 - auc_97: 0.9506 - val_loss: 0.7481 - val_auc_97: 0.9268\nEpoch 27/50\n14/14 [==============================] - 0s 6ms/step - loss: 0.6412 - auc_97: 0.9553 - val_loss: 0.8005 - val_auc_97: 0.9221\nEpoch 28/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6264 - auc_97: 0.9558 - val_loss: 0.7578 - val_auc_97: 0.9319\nEpoch 29/50\n14/14 [==============================] - 0s 6ms/step - loss: 0.5930 - auc_97: 0.9603 - val_loss: 0.8462 - val_auc_97: 0.9231\nEpoch 30/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6109 - auc_97: 0.9570 - val_loss: 0.7528 - val_auc_97: 0.9311\nEpoch 31/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6074 - auc_97: 0.9590 - val_loss: 0.6896 - val_auc_97: 0.9434\nEpoch 32/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6351 - auc_97: 0.9549 - val_loss: 0.7250 - val_auc_97: 0.9389\nEpoch 33/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6798 - auc_97: 0.9483 - val_loss: 0.7253 - val_auc_97: 0.9329\nEpoch 34/50\n14/14 [==============================] - 0s 6ms/step - loss: 0.6334 - auc_97: 0.9555 - val_loss: 0.6711 - val_auc_97: 0.9443\nEpoch 35/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6322 - auc_97: 0.9561 - val_loss: 0.7219 - val_auc_97: 0.9413\nEpoch 36/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6250 - auc_97: 0.9560 - val_loss: 0.6441 - val_auc_97: 0.9492\nEpoch 37/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6587 - auc_97: 0.9518 - val_loss: 0.6430 - val_auc_97: 0.9475\nEpoch 38/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6535 - auc_97: 0.9501 - val_loss: 0.6120 - val_auc_97: 0.9552\nEpoch 39/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6583 - auc_97: 0.9516 - val_loss: 0.6565 - val_auc_97: 0.9460\nEpoch 40/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.6448 - auc_97: 0.9537 - val_loss: 0.6333 - val_auc_97: 0.9509\nEpoch 41/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6238 - auc_97: 0.9563 - val_loss: 0.6419 - val_auc_97: 0.9439\nEpoch 42/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5939 - auc_97: 0.9597 - val_loss: 0.6642 - val_auc_97: 0.9415\nEpoch 43/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6009 - auc_97: 0.9587 - val_loss: 0.7290 - val_auc_97: 0.9354\nEpoch 44/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6092 - auc_97: 0.9578 - val_loss: 0.7460 - val_auc_97: 0.9288\nEpoch 45/50\n14/14 [==============================] - 0s 2ms/step - loss: 0.5868 - auc_97: 0.9608 - val_loss: 0.6142 - val_auc_97: 0.9522\nEpoch 46/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6671 - auc_97: 0.9508 - val_loss: 0.6134 - val_auc_97: 0.9547\nEpoch 47/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6322 - auc_97: 0.9562 - val_loss: 0.6152 - val_auc_97: 0.9496\nEpoch 48/50\n14/14 [==============================] - 0s 5ms/step - loss: 0.6551 - auc_97: 0.9517 - val_loss: 0.5831 - val_auc_97: 0.9578\nEpoch 49/50\n14/14 [==============================] - 0s 3ms/step - loss: 0.6389 - auc_97: 0.9527 - val_loss: 0.7512 - val_auc_97: 0.9267\nEpoch 50/50\n14/14 [==============================] - 0s 4ms/step - loss: 0.6227 - auc_97: 0.9540 - val_loss: 0.7196 - val_auc_97: 0.9308\n[I 2024-09-30 13:08:31,675] Trial 96 finished with value: 0.9511499404907227 and parameters: {'num_layers': 3, 'num_neurons': 29, 'learning_rate': 0.009040224690641066, 'batch_size': 27}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n12/12 [==============================] - 1s 18ms/step - loss: 1.8126 - auc_98: 0.8292 - val_loss: 1.4549 - val_auc_98: 0.8534\nEpoch 2/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.8838 - auc_98: 0.9240 - val_loss: 1.1075 - val_auc_98: 0.9145\nEpoch 3/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.7523 - auc_98: 0.9429 - val_loss: 1.1804 - val_auc_98: 0.8899\nEpoch 4/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.6498 - auc_98: 0.9543 - val_loss: 0.8707 - val_auc_98: 0.9233\nEpoch 5/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.6138 - auc_98: 0.9585 - val_loss: 0.8604 - val_auc_98: 0.9250\nEpoch 6/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5786 - auc_98: 0.9626 - val_loss: 0.8825 - val_auc_98: 0.9249\nEpoch 7/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.6512 - auc_98: 0.9542 - val_loss: 0.8398 - val_auc_98: 0.9306\nEpoch 8/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.6275 - auc_98: 0.9569 - val_loss: 0.7765 - val_auc_98: 0.9332\nEpoch 9/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.6248 - auc_98: 0.9565 - val_loss: 0.9467 - val_auc_98: 0.9142\nEpoch 10/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.6328 - auc_98: 0.9573 - val_loss: 0.9178 - val_auc_98: 0.9151\nEpoch 11/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.6164 - auc_98: 0.9566 - val_loss: 0.9088 - val_auc_98: 0.9201\nEpoch 12/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.6426 - auc_98: 0.9553 - val_loss: 0.9441 - val_auc_98: 0.9227\nEpoch 13/50\n12/12 [==============================] - 0s 4ms/step - loss: 0.6101 - auc_98: 0.9588 - val_loss: 0.9579 - val_auc_98: 0.9170\nEpoch 14/50\n12/12 [==============================] - 0s 2ms/step - loss: 0.6186 - auc_98: 0.9582 - val_loss: 0.7159 - val_auc_98: 0.9414\nEpoch 15/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5782 - auc_98: 0.9627 - val_loss: 0.7050 - val_auc_98: 0.9422\nEpoch 16/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5683 - auc_98: 0.9633 - val_loss: 0.6699 - val_auc_98: 0.9475\nEpoch 17/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.5774 - auc_98: 0.9618 - val_loss: 0.6845 - val_auc_98: 0.9454\nEpoch 18/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5808 - auc_98: 0.9597 - val_loss: 0.7476 - val_auc_98: 0.9401\nEpoch 19/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5340 - auc_98: 0.9669 - val_loss: 0.6936 - val_auc_98: 0.9406\nEpoch 20/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5371 - auc_98: 0.9671 - val_loss: 0.9177 - val_auc_98: 0.9239\nEpoch 21/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5538 - auc_98: 0.9657 - val_loss: 0.8407 - val_auc_98: 0.9298\nEpoch 22/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5505 - auc_98: 0.9649 - val_loss: 0.7087 - val_auc_98: 0.9389\nEpoch 23/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.5540 - auc_98: 0.9645 - val_loss: 0.9243 - val_auc_98: 0.9157\nEpoch 24/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.6114 - auc_98: 0.9581 - val_loss: 0.9349 - val_auc_98: 0.9149\nEpoch 25/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5716 - auc_98: 0.9630 - val_loss: 0.9166 - val_auc_98: 0.9214\nEpoch 26/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5667 - auc_98: 0.9643 - val_loss: 0.7218 - val_auc_98: 0.9436\nEpoch 27/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5640 - auc_98: 0.9631 - val_loss: 0.7998 - val_auc_98: 0.9361\nEpoch 28/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.5721 - auc_98: 0.9633 - val_loss: 0.8009 - val_auc_98: 0.9276\nEpoch 29/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5681 - auc_98: 0.9628 - val_loss: 0.7516 - val_auc_98: 0.9354\nEpoch 30/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.5931 - auc_98: 0.9595 - val_loss: 1.0654 - val_auc_98: 0.9084\nEpoch 31/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5787 - auc_98: 0.9619 - val_loss: 0.6673 - val_auc_98: 0.9453\nEpoch 32/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5370 - auc_98: 0.9672 - val_loss: 0.6659 - val_auc_98: 0.9479\nEpoch 33/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5476 - auc_98: 0.9659 - val_loss: 0.9323 - val_auc_98: 0.9213\nEpoch 34/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.6118 - auc_98: 0.9578 - val_loss: 0.8538 - val_auc_98: 0.9309\nEpoch 35/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5349 - auc_98: 0.9679 - val_loss: 0.7293 - val_auc_98: 0.9426\nEpoch 36/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.5418 - auc_98: 0.9658 - val_loss: 0.6849 - val_auc_98: 0.9487\nEpoch 37/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5256 - auc_98: 0.9687 - val_loss: 0.7132 - val_auc_98: 0.9450\nEpoch 38/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5548 - auc_98: 0.9654 - val_loss: 0.7784 - val_auc_98: 0.9373\nEpoch 39/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5546 - auc_98: 0.9641 - val_loss: 0.7041 - val_auc_98: 0.9448\nEpoch 40/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5800 - auc_98: 0.9622 - val_loss: 0.8990 - val_auc_98: 0.9251\nEpoch 41/50\n12/12 [==============================] - 0s 4ms/step - loss: 0.5505 - auc_98: 0.9660 - val_loss: 0.8867 - val_auc_98: 0.9270\nEpoch 42/50\n12/12 [==============================] - 0s 5ms/step - loss: 0.5778 - auc_98: 0.9626 - val_loss: 0.6812 - val_auc_98: 0.9464\nEpoch 43/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5244 - auc_98: 0.9689 - val_loss: 0.7128 - val_auc_98: 0.9482\nEpoch 44/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5113 - auc_98: 0.9698 - val_loss: 0.6787 - val_auc_98: 0.9498\nEpoch 45/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5314 - auc_98: 0.9669 - val_loss: 0.7535 - val_auc_98: 0.9434\nEpoch 46/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5448 - auc_98: 0.9660 - val_loss: 0.8339 - val_auc_98: 0.9301\nEpoch 47/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.5431 - auc_98: 0.9658 - val_loss: 0.8276 - val_auc_98: 0.9341\nEpoch 48/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.6124 - auc_98: 0.9582 - val_loss: 0.6440 - val_auc_98: 0.9504\nEpoch 49/50\n12/12 [==============================] - 0s 6ms/step - loss: 0.6246 - auc_98: 0.9564 - val_loss: 0.7568 - val_auc_98: 0.9445\nEpoch 50/50\n12/12 [==============================] - 0s 3ms/step - loss: 0.5734 - auc_98: 0.9629 - val_loss: 0.7249 - val_auc_98: 0.9450\n[I 2024-09-30 13:08:35,017] Trial 97 finished with value: 0.9655599594116211 and parameters: {'num_layers': 3, 'num_neurons': 43, 'learning_rate': 0.005217430302484757, 'batch_size': 31}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n13/13 [==============================] - 1s 17ms/step - loss: 4.1160 - auc_99: 0.5516 - val_loss: 1.9116 - val_auc_99: 0.5922\nEpoch 2/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.7846 - auc_99: 0.6293 - val_loss: 1.7302 - val_auc_99: 0.6577\nEpoch 3/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.6602 - auc_99: 0.6936 - val_loss: 1.6442 - val_auc_99: 0.7540\nEpoch 4/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.6314 - auc_99: 0.7479 - val_loss: 1.6021 - val_auc_99: 0.7897\nEpoch 5/50\n13/13 [==============================] - 0s 2ms/step - loss: 1.5899 - auc_99: 0.7559 - val_loss: 1.6142 - val_auc_99: 0.7481\nEpoch 6/50\n13/13 [==============================] - 0s 5ms/step - loss: 1.5789 - auc_99: 0.7668 - val_loss: 1.5395 - val_auc_99: 0.7982\nEpoch 7/50\n13/13 [==============================] - 0s 2ms/step - loss: 1.5239 - auc_99: 0.7720 - val_loss: 1.4439 - val_auc_99: 0.8046\nEpoch 8/50\n13/13 [==============================] - 0s 5ms/step - loss: 1.4272 - auc_99: 0.8085 - val_loss: 1.4168 - val_auc_99: 0.8213\nEpoch 9/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.3406 - auc_99: 0.8378 - val_loss: 1.2627 - val_auc_99: 0.8737\nEpoch 10/50\n13/13 [==============================] - 0s 5ms/step - loss: 1.1965 - auc_99: 0.8813 - val_loss: 1.1225 - val_auc_99: 0.9012\nEpoch 11/50\n13/13 [==============================] - 0s 3ms/step - loss: 1.0941 - auc_99: 0.9004 - val_loss: 1.0441 - val_auc_99: 0.8961\nEpoch 12/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.0057 - auc_99: 0.9089 - val_loss: 0.9558 - val_auc_99: 0.9280\nEpoch 13/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.8992 - auc_99: 0.9321 - val_loss: 0.8519 - val_auc_99: 0.9377\nEpoch 14/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.8157 - auc_99: 0.9416 - val_loss: 0.8483 - val_auc_99: 0.9316\nEpoch 15/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.8041 - auc_99: 0.9378 - val_loss: 0.8737 - val_auc_99: 0.9271\nEpoch 16/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.7523 - auc_99: 0.9467 - val_loss: 0.8275 - val_auc_99: 0.9313\nEpoch 17/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6989 - auc_99: 0.9547 - val_loss: 0.7936 - val_auc_99: 0.9358\nEpoch 18/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7052 - auc_99: 0.9534 - val_loss: 0.8487 - val_auc_99: 0.9273\nEpoch 19/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.6781 - auc_99: 0.9544 - val_loss: 0.7682 - val_auc_99: 0.9367\nEpoch 20/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6290 - auc_99: 0.9624 - val_loss: 0.7329 - val_auc_99: 0.9403\nEpoch 21/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5945 - auc_99: 0.9692 - val_loss: 0.7040 - val_auc_99: 0.9454\nEpoch 22/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5763 - auc_99: 0.9692 - val_loss: 0.7444 - val_auc_99: 0.9398\nEpoch 23/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.6089 - auc_99: 0.9620 - val_loss: 0.7003 - val_auc_99: 0.9432\nEpoch 24/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.5998 - auc_99: 0.9632 - val_loss: 0.7590 - val_auc_99: 0.9377\nEpoch 25/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5775 - auc_99: 0.9660 - val_loss: 0.7178 - val_auc_99: 0.9417\nEpoch 26/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5663 - auc_99: 0.9672 - val_loss: 0.6624 - val_auc_99: 0.9500\nEpoch 27/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5386 - auc_99: 0.9707 - val_loss: 0.8903 - val_auc_99: 0.9174\nEpoch 28/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5565 - auc_99: 0.9670 - val_loss: 0.7444 - val_auc_99: 0.9382\nEpoch 29/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5260 - auc_99: 0.9712 - val_loss: 0.7116 - val_auc_99: 0.9456\nEpoch 30/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5363 - auc_99: 0.9690 - val_loss: 0.6911 - val_auc_99: 0.9474\nEpoch 31/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5059 - auc_99: 0.9739 - val_loss: 0.7479 - val_auc_99: 0.9390\nEpoch 32/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5138 - auc_99: 0.9714 - val_loss: 0.6728 - val_auc_99: 0.9476\nEpoch 33/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5393 - auc_99: 0.9678 - val_loss: 0.7940 - val_auc_99: 0.9373\nEpoch 34/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.4851 - auc_99: 0.9748 - val_loss: 0.7468 - val_auc_99: 0.9416\nEpoch 35/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.4869 - auc_99: 0.9744 - val_loss: 0.7499 - val_auc_99: 0.9433\nEpoch 36/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.4968 - auc_99: 0.9724 - val_loss: 0.8743 - val_auc_99: 0.9219\nEpoch 37/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5065 - auc_99: 0.9716 - val_loss: 0.8725 - val_auc_99: 0.9169\nEpoch 38/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5235 - auc_99: 0.9684 - val_loss: 0.7620 - val_auc_99: 0.9388\nEpoch 39/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5888 - auc_99: 0.9602 - val_loss: 0.7005 - val_auc_99: 0.9444\nEpoch 40/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5185 - auc_99: 0.9691 - val_loss: 0.7269 - val_auc_99: 0.9380\nEpoch 41/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5106 - auc_99: 0.9698 - val_loss: 0.7291 - val_auc_99: 0.9437\nEpoch 42/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5510 - auc_99: 0.9651 - val_loss: 0.7564 - val_auc_99: 0.9343\nEpoch 43/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5185 - auc_99: 0.9682 - val_loss: 0.7319 - val_auc_99: 0.9398\nEpoch 44/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.4554 - auc_99: 0.9780 - val_loss: 0.7261 - val_auc_99: 0.9464\nEpoch 45/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.5053 - auc_99: 0.9701 - val_loss: 0.8001 - val_auc_99: 0.9327\nEpoch 46/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.4425 - auc_99: 0.9789 - val_loss: 0.6913 - val_auc_99: 0.9470\nEpoch 47/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.4362 - auc_99: 0.9792 - val_loss: 0.7643 - val_auc_99: 0.9348\nEpoch 48/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.4605 - auc_99: 0.9758 - val_loss: 0.8075 - val_auc_99: 0.9321\nEpoch 49/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.4542 - auc_99: 0.9772 - val_loss: 0.7882 - val_auc_99: 0.9303\nEpoch 50/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.4358 - auc_99: 0.9784 - val_loss: 0.7281 - val_auc_99: 0.9424\n[I 2024-09-30 13:08:38,353] Trial 98 finished with value: 0.9766601324081421 and parameters: {'num_layers': 3, 'num_neurons': 32, 'learning_rate': 0.004311950271898015, 'batch_size': 28}. Best is trial 53 with value: 0.9809099435806274.\nEpoch 1/50\n/tmp/ipykernel_8770/79630627.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n13/13 [==============================] - 1s 17ms/step - loss: 6.7715 - auc_100: 0.5930 - val_loss: 2.6824 - val_auc_100: 0.7747\nEpoch 2/50\n13/13 [==============================] - 0s 4ms/step - loss: 1.9110 - auc_100: 0.8436 - val_loss: 1.4141 - val_auc_100: 0.8737\nEpoch 3/50\n13/13 [==============================] - 0s 2ms/step - loss: 1.2461 - auc_100: 0.9024 - val_loss: 1.1858 - val_auc_100: 0.8954\nEpoch 4/50\n13/13 [==============================] - 0s 5ms/step - loss: 1.0613 - auc_100: 0.9173 - val_loss: 1.1363 - val_auc_100: 0.8974\nEpoch 5/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.8836 - auc_100: 0.9302 - val_loss: 0.9165 - val_auc_100: 0.9157\nEpoch 6/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.7995 - auc_100: 0.9361 - val_loss: 0.9161 - val_auc_100: 0.9143\nEpoch 7/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.7081 - auc_100: 0.9504 - val_loss: 0.8713 - val_auc_100: 0.9265\nEpoch 8/50\n13/13 [==============================] - 0s 8ms/step - loss: 0.6421 - auc_100: 0.9580 - val_loss: 0.7359 - val_auc_100: 0.9402\nEpoch 9/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6214 - auc_100: 0.9592 - val_loss: 0.8183 - val_auc_100: 0.9305\nEpoch 10/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6086 - auc_100: 0.9608 - val_loss: 0.7959 - val_auc_100: 0.9313\nEpoch 11/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6122 - auc_100: 0.9593 - val_loss: 0.7893 - val_auc_100: 0.9352\nEpoch 12/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6162 - auc_100: 0.9584 - val_loss: 0.6504 - val_auc_100: 0.9475\nEpoch 13/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.6167 - auc_100: 0.9581 - val_loss: 0.7007 - val_auc_100: 0.9390\nEpoch 14/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5875 - auc_100: 0.9617 - val_loss: 0.8003 - val_auc_100: 0.9331\nEpoch 15/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5856 - auc_100: 0.9623 - val_loss: 0.7042 - val_auc_100: 0.9391\nEpoch 16/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5833 - auc_100: 0.9626 - val_loss: 0.6724 - val_auc_100: 0.9458\nEpoch 17/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5620 - auc_100: 0.9645 - val_loss: 0.7786 - val_auc_100: 0.9358\nEpoch 18/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5664 - auc_100: 0.9643 - val_loss: 0.7669 - val_auc_100: 0.9388\nEpoch 19/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5780 - auc_100: 0.9632 - val_loss: 0.8589 - val_auc_100: 0.9284\nEpoch 20/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6116 - auc_100: 0.9586 - val_loss: 0.7434 - val_auc_100: 0.9352\nEpoch 21/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5844 - auc_100: 0.9626 - val_loss: 0.8090 - val_auc_100: 0.9330\nEpoch 22/50\n13/13 [==============================] - 0s 7ms/step - loss: 0.6021 - auc_100: 0.9605 - val_loss: 0.6871 - val_auc_100: 0.9442\nEpoch 23/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.6056 - auc_100: 0.9594 - val_loss: 0.8063 - val_auc_100: 0.9374\nEpoch 24/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6105 - auc_100: 0.9596 - val_loss: 0.8453 - val_auc_100: 0.9319\nEpoch 25/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5774 - auc_100: 0.9636 - val_loss: 0.6629 - val_auc_100: 0.9453\nEpoch 26/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5647 - auc_100: 0.9641 - val_loss: 0.7614 - val_auc_100: 0.9379\nEpoch 27/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.5593 - auc_100: 0.9647 - val_loss: 0.7159 - val_auc_100: 0.9383\nEpoch 28/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5589 - auc_100: 0.9654 - val_loss: 1.0403 - val_auc_100: 0.9146\nEpoch 29/50\n13/13 [==============================] - 0s 6ms/step - loss: 0.6336 - auc_100: 0.9573 - val_loss: 0.7978 - val_auc_100: 0.9361\nEpoch 30/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5943 - auc_100: 0.9597 - val_loss: 0.7948 - val_auc_100: 0.9326\nEpoch 31/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5852 - auc_100: 0.9618 - val_loss: 0.6696 - val_auc_100: 0.9466\nEpoch 32/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.6196 - auc_100: 0.9587 - val_loss: 0.6201 - val_auc_100: 0.9543\nEpoch 33/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5743 - auc_100: 0.9633 - val_loss: 0.8327 - val_auc_100: 0.9321\nEpoch 34/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.5972 - auc_100: 0.9604 - val_loss: 0.6299 - val_auc_100: 0.9509\nEpoch 35/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.5892 - auc_100: 0.9617 - val_loss: 0.7386 - val_auc_100: 0.9406\nEpoch 36/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6491 - auc_100: 0.9555 - val_loss: 0.6459 - val_auc_100: 0.9515\nEpoch 37/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6152 - auc_100: 0.9583 - val_loss: 0.7095 - val_auc_100: 0.9400\nEpoch 38/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5781 - auc_100: 0.9625 - val_loss: 0.7920 - val_auc_100: 0.9348\nEpoch 39/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6061 - auc_100: 0.9597 - val_loss: 0.7038 - val_auc_100: 0.9435\nEpoch 40/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.6157 - auc_100: 0.9584 - val_loss: 0.6376 - val_auc_100: 0.9533\nEpoch 41/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.6324 - auc_100: 0.9568 - val_loss: 1.0514 - val_auc_100: 0.9141\nEpoch 42/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5900 - auc_100: 0.9622 - val_loss: 0.8519 - val_auc_100: 0.9294\nEpoch 43/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5505 - auc_100: 0.9668 - val_loss: 0.7168 - val_auc_100: 0.9406\nEpoch 44/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.5408 - auc_100: 0.9673 - val_loss: 0.6994 - val_auc_100: 0.9471\nEpoch 45/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.5689 - auc_100: 0.9636 - val_loss: 0.7437 - val_auc_100: 0.9409\nEpoch 46/50\n13/13 [==============================] - 0s 2ms/step - loss: 0.5355 - auc_100: 0.9669 - val_loss: 0.8124 - val_auc_100: 0.9314\nEpoch 47/50\n13/13 [==============================] - 0s 5ms/step - loss: 0.5417 - auc_100: 0.9669 - val_loss: 0.7252 - val_auc_100: 0.9393\nEpoch 48/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5576 - auc_100: 0.9643 - val_loss: 0.7853 - val_auc_100: 0.9334\nEpoch 49/50\n13/13 [==============================] - 0s 4ms/step - loss: 0.5656 - auc_100: 0.9639 - val_loss: 0.6908 - val_auc_100: 0.9451\nEpoch 50/50\n13/13 [==============================] - 0s 3ms/step - loss: 0.5669 - auc_100: 0.9646 - val_loss: 0.7049 - val_auc_100: 0.9469\n[I 2024-09-30 13:08:41,612] Trial 99 finished with value: 0.9600299596786499 and parameters: {'num_layers': 3, 'num_neurons': 27, 'learning_rate': 0.0017550254569285342, 'batch_size': 28}. Best is trial 53 with value: 0.9809099435806274.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/b136087f-1fb7-43cc-887c-323a7e4c7f2e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d10e5816","execution_start":1727701847913,"execution_millis":0,"execution_context_id":"a0dd2ee3-88e6-474d-a014-eaa290f23e0b","deepnote_app_block_visible":true,"cell_id":"2b195c4801084cf490bce2de7476977a","deepnote_cell_type":"code"},"source":"print(study.best_trial.params)","block_group":"33515716aa764b488a6eb35b1bfe853b","execution_count":11,"outputs":[{"name":"stdout","text":"{'num_layers': 3, 'num_neurons': 45, 'learning_rate': 0.005704347102265108, 'batch_size': 28}\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/0e8d2bbd-ba24-48a4-9e1b-2f4a6d86e42b","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"d4d32d79a770485185ba96a2580f7748","deepnote_cell_type":"text-cell-p"},"source":"De beste uitkomst is: 3 layers, 45 neurons per layer, learning rate van 0.057, batch size van 28","block_group":"3f6ea1c640f54e72920178ceecc5c9f8"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=70241bed-abd0-479d-8376-545af7db2871' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_app_layout":"powerful-article","deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"9aff531222ad4808a59bb0e98174cbd0","deepnote_execution_queue":[]}}